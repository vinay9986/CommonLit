{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tldextract\n!pip install sweetviz","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-25T21:50:12.496355Z","iopub.execute_input":"2021-05-25T21:50:12.496917Z","iopub.status.idle":"2021-05-25T21:50:32.077103Z","shell.execute_reply.started":"2021-05-25T21:50:12.496806Z","shell.execute_reply":"2021-05-25T21:50:32.075476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing Packages\nimport pandas as pd\nimport numpy as np\nimport spacy\nimport sys\nsys.path = [\n    '../input/readability-package/',\n] + sys.path\nimport readability\nimport nltk\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nfrom nltk import pos_tag, pos_tag_sents\nfrom urllib.parse import urlparse\nimport re\nfrom tldextract import extract\n\nfrom sklearn import metrics, preprocessing, model_selection\nimport lightgbm as lgb\nimport copy\nimport sweetviz as sv\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport random","metadata":{"execution":{"iopub.status.busy":"2021-05-25T21:50:32.080367Z","iopub.execute_input":"2021-05-25T21:50:32.080942Z","iopub.status.idle":"2021-05-25T21:50:36.425472Z","shell.execute_reply.started":"2021-05-25T21:50:32.080877Z","shell.execute_reply":"2021-05-25T21:50:36.424201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reading Data\npd.options.display.max_rows = 4000\ntrain = pd.read_csv('../input/commonlitreadabilityprize/train.csv', low_memory=False)\ntest = pd.read_csv('../input/commonlitreadabilityprize/test.csv', low_memory=False)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T21:50:36.427851Z","iopub.execute_input":"2021-05-25T21:50:36.428314Z","iopub.status.idle":"2021-05-25T21:50:36.549983Z","shell.execute_reply.started":"2021-05-25T21:50:36.428259Z","shell.execute_reply":"2021-05-25T21:50:36.548724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Taken this from https://www.kaggle.com/ravishah1/readability-feature-engineering-non-nn-baseline\ndef readability_measurements(passage: str):\n    \"\"\"\n    This function uses the readability library for feature engineering.\n    It includes textual statistics, readability scales and metric, and some pos stats\n    \"\"\"\n    results = readability.getmeasures(passage, lang='en')\n    \n    chars_per_word = results['sentence info']['characters_per_word']\n    syll_per_word = results['sentence info']['syll_per_word']\n    words_per_sent = results['sentence info']['words_per_sentence']\n    \n    kincaid = results['readability grades']['Kincaid']\n    ari = results['readability grades']['ARI']\n    coleman_liau = results['readability grades']['Coleman-Liau']\n    flesch = results['readability grades']['FleschReadingEase']\n    gunning_fog = results['readability grades']['GunningFogIndex']\n    lix = results['readability grades']['LIX']\n    smog = results['readability grades']['SMOGIndex']\n    rix = results['readability grades']['RIX']\n    dale_chall = results['readability grades']['DaleChallIndex']\n    \n    tobeverb = results['word usage']['tobeverb']\n    auxverb = results['word usage']['auxverb']\n    conjunction = results['word usage']['conjunction']\n    pronoun = results['word usage']['pronoun']\n    preposition = results['word usage']['preposition']\n    nominalization = results['word usage']['nominalization']\n    \n    pronoun_b = results['sentence beginnings']['pronoun']\n    interrogative = results['sentence beginnings']['interrogative']\n    article = results['sentence beginnings']['article']\n    subordination = results['sentence beginnings']['subordination']\n    conjunction_b = results['sentence beginnings']['conjunction']\n    preposition_b = results['sentence beginnings']['preposition']\n\n    \n    return [chars_per_word, syll_per_word, words_per_sent,\n            kincaid, ari, coleman_liau, flesch, gunning_fog, lix, smog, rix, dale_chall,\n            tobeverb, auxverb, conjunction, pronoun, preposition, nominalization,\n            pronoun_b, interrogative, article, subordination, conjunction_b, preposition_b]\n\n# Taken this from https://www.kaggle.com/ravishah1/readability-feature-engineering-non-nn-baseline\ndef spacy_features(df: pd.DataFrame):\n    \"\"\"\n    This function generates features using spacy en_core_wb_lg\n    I learned about this from these resources:\n    https://www.kaggle.com/konradb/linear-baseline-with-cv\n    https://www.kaggle.com/anaverageengineer/comlrp-baseline-for-complete-beginners\n    \"\"\"\n    \n    nlp = spacy.load('en_core_web_lg')\n    with nlp.disable_pipes():\n        vectors = np.array([nlp(text).vector for text in df.excerpt])\n        \n    return vectors\n\ndef get_spacy_col_names():\n    names = list()\n    for i in range(300):\n        names.append(f\"spacy_{i}\")\n        \n    return names\n\n# Taken this from https://www.kaggle.com/ravishah1/readability-feature-engineering-non-nn-baseline\ndef pos_tag_features(passage: str):\n    \"\"\"\n    This function counts the number of times different parts of speech occur in an excerpt\n    \"\"\"\n    pos_tags = [\"CC\", \"CD\", \"DT\", \"EX\", \"FW\", \"IN\", \"JJ\", \"JJR\", \"JJS\", \"LS\", \"MD\", \n                \"NN\", \"NNS\", \"NNP\", \"NNPS\", \"PDT\", \"POS\", \"PRP\", \"RB\", \"RBR\", \"RBS\", \"RP\", \"TO\", \"UH\",\n                \"VB\", \"VBD\", \"VBG\", \"VBZ\", \"WDT\", \"WP\", \"WRB\"]\n    \n    tags = pos_tag(word_tokenize(passage))\n    tag_list= list()\n    \n    for tag in pos_tags:\n        tag_list.append(len([i[0] for i in tags if i[1] == tag]))\n    \n    return tag_list\n\n# Taken this from https://www.kaggle.com/ravishah1/readability-feature-engineering-non-nn-baseline\ndef generate_other_features(passage: str):\n    \"\"\"\n    This function is where I test miscellaneous features\n    This is experimental\n    \"\"\"\n    # punctuation count\n    periods = passage.count(\".\")\n    commas = passage.count(\",\")\n    semis = passage.count(\";\")\n    exclaims = passage.count(\"!\")\n    questions = passage.count(\"?\")\n    \n    # Some other stats\n    num_char = len(passage)\n    num_words = len(passage.split(\" \"))\n    unique_words = len(set(passage.split(\" \") ))\n    word_diversity = unique_words/num_words\n    \n    word_len = [len(w) for w in passage.split(\" \")]\n    longest_word = np.max(word_len)\n    avg_len_word = np.mean(word_len)\n    \n    return [periods, commas, semis, exclaims, questions,\n            num_char, num_words, unique_words, word_diversity,\n            longest_word, avg_len_word]\n\ndef extract_features(df):\n\n    scores_df = pd.DataFrame(df[\"excerpt\"].apply(lambda p : readability_measurements(p)).tolist(), \n                                 columns=[\"chars_per_word\", \"syll_per_word\", \"words_per_sent\",\n                                          \"kincaid\", \"ari\", \"coleman_liau\", \"flesch\", \"gunning_fog\", \"lix\", \"smog\", \"rix\", \"dale_chall\",\n                                          \"tobeverb\", \"auxverb\", \"conjunction\", \"pronoun\", \"preposition\", \"nominalization\",\n                                          \"pronoun_b\", \"interrogative\", \"article\", \"subordination\", \"conjunction_b\", \"preposition_b\"])\n    df = pd.merge(df, scores_df, left_index=True, right_index=True)\n    \n    spacy_df = pd.DataFrame(spacy_features(df), columns=get_spacy_col_names())\n    df = pd.merge(df, spacy_df, left_index=True, right_index=True)\n    \n    pos_df = pd.DataFrame(df[\"excerpt\"].apply(lambda p : pos_tag_features(p)).tolist(),\n                            columns=[\"CC\", \"CD\", \"DT\", \"EX\", \"FW\", \"IN\", \"JJ\", \"JJR\", \"JJS\", \"LS\", \"MD\", \n                                    \"NN\", \"NNS\", \"NNP\", \"NNPS\", \"PDT\", \"POS\", \"PRP\", \"RB\", \"RBR\", \"RBS\", \"RP\", \"TO\", \"UH\",\n                                    \"VB\", \"VBD\", \"VBG\", \"VBZ\", \"WDT\", \"WP\", \"WRB\"])\n    df = pd.merge(df, pos_df, left_index=True, right_index=True)\n    \n    other_df = pd.DataFrame(df[\"excerpt\"].apply(lambda p : generate_other_features(p)).tolist(),\n                            columns=[\"periods\", \"commas\", \"semis\", \"exclaims\", \"questions\",\n                                        \"num_char\", \"num_words\", \"unique_words\", \"word_diversity\",\n                                        \"longest_word\", \"avg_len_word\"])\n    df = pd.merge(df, other_df, left_index=True, right_index=True)\n\n    return df\n\ndef extract_url_license_feat(df):\n    temp = pd.DataFrame()\n    temp['article_year'] = df['url_legal'].apply(lambda x : x if x is np.nan else re.search('(2\\d{3})|$', urlparse(x).path).group())\n    temp['subdomain'] = df['url_legal'].apply(lambda x : x if x is np.nan else extract(x)[0])\n    temp['domain'] = df['url_legal'].apply(lambda x : x if x is np.nan else extract(x)[1])\n    temp['suffix'] = df['url_legal'].apply(lambda x : x if x is np.nan else extract(x)[2])\n    temp['is_pdf'] = df['url_legal'].apply(lambda x : x if x is np.nan else ('Y' if '.pdf' in str(x) else 'N'))\n    \n    temp['is_cc'] = df['license'].apply(lambda x : x if x is np.nan else ('Y' if 'CC' in str(x) else 'N'))\n    temp['is_by'] = df['license'].apply(lambda x : x if x is np.nan else ('Y' if 'BY' in str(x) else 'N'))\n    temp['is_sa'] = df['license'].apply(lambda x : x if x is np.nan else ('Y' if 'SA' in str(x) else 'N'))\n    temp['is_nc'] = df['license'].apply(lambda x : x if x is np.nan else ('Y' if 'NC' in str(x) else 'N'))\n    temp['is_nd'] = df['license'].apply(lambda x : x if x is np.nan else ('Y' if 'ND' in str(x) else 'N'))\n    temp['is_gnu'] = df['license'].apply(lambda x : x if x is np.nan else ('Y' if 'GNU' in str(x) else 'N'))\n    temp['license_version'] = df['license'].apply(lambda x : x if x is np.nan else(float(0) if re.search('([0-9][.][0-9])|$', urlparse(x).path).group() == '' else float(re.search('([0-9][.][0-9])|$', urlparse(x).path).group())))\n    df = pd.concat([df, temp], axis = 1)\n    return df\n\ndef handle_cate_NA(df, columns_to_ignore=[]):\n    temp = copy.deepcopy(df)\n    cate_cols = list(set(temp.select_dtypes('object').columns.tolist()) - set(columns_to_ignore))\n    for col in cate_cols:\n        if temp[col].isna().sum() > 0:\n            column_name = 'NA_POS_'+col\n            col_values = ['Y' if pd.isna(value[1]) else 'N' for value in df[col].items()]\n            temp[col].fillna(value='ABS', inplace=True)\n            temp[column_name] = col_values\n    return temp\n\ndef handle_cont_NA(df, method='mean'):\n    action = ''.join(c.lower() for c in method if not c.isspace())\n    temp = copy.deepcopy(df)\n    num_cols = temp.select_dtypes(include='number')\n    for col in num_cols:\n        if temp[col].isna().sum() > 0:\n            column_name = 'NA_POS_'+col\n            col_values = ['Y' if pd.isna(value[1]) else 'N' for value in df[col].items()]\n            #value_if_true if condition else value_if_false\n            fill_value = np.mean(temp[col]) if 'mean' == action else np.median(temp[col])\n            temp[col].fillna(value = fill_value, inplace=True)\n            temp[column_name] = col_values\n    return temp\n\ndef train_pca(df, list_of_columns, column_prefix):\n    temp = copy.deepcopy(df)\n    x = temp.loc[:, list_of_columns].values\n    ss = StandardScaler().fit(x)\n    x = ss.transform(x)\n    pca = PCA(n_components=2)\n    pca.fit(x)\n    principalComponents = pca.transform(x)\n    print(column_prefix, pca.explained_variance_ratio_)\n    principalDf = pd.DataFrame(data = principalComponents, columns = [column_prefix+'_1', column_prefix+'_2'])\n#     temp.drop(columns=list_of_columns, axis=1, inplace=True)\n    temp = pd.concat([temp, principalDf], axis = 1)\n    result_dict = { 'pca': pca, 'ss': ss, 'list_of_columns': list_of_columns, 'column_prefix': column_prefix } \n    return result_dict, temp\n\ndef apply_pca(trained_pca, df):\n    temp = copy.deepcopy(df)\n    x = temp.loc[:, trained_pca.get('list_of_columns')].values\n    x = trained_pca.get('ss').transform(x)\n    principalComponents = trained_pca.get('pca').transform(x)\n    principalDf = pd.DataFrame(data = principalComponents, columns = [trained_pca.get('column_prefix')+'_1', trained_pca.get('column_prefix')+'_2'])\n#     temp.drop(columns=trained_pca.get('list_of_columns'), axis=1, inplace=True)\n    temp = pd.concat([temp, principalDf], axis = 1)\n    return temp","metadata":{"execution":{"iopub.status.busy":"2021-05-25T21:50:36.55304Z","iopub.execute_input":"2021-05-25T21:50:36.553586Z","iopub.status.idle":"2021-05-25T21:50:36.606072Z","shell.execute_reply.started":"2021-05-25T21:50:36.553521Z","shell.execute_reply":"2021-05-25T21:50:36.604905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_feat = extract_features(train)\n# train_feat = extract_url_license_feat(train_feat)\n# train_feat = handle_cate_NA(train_feat)\n# train_feat = handle_cont_NA(train_feat)\n# train_feat.head()\n\ntrain_feat = pd.read_csv('../input/commonlitfe/train_feat.csv', low_memory=False)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T21:50:36.607588Z","iopub.execute_input":"2021-05-25T21:50:36.607922Z","iopub.status.idle":"2021-05-25T21:50:37.24011Z","shell.execute_reply.started":"2021-05-25T21:50:36.607889Z","shell.execute_reply":"2021-05-25T21:50:37.238773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_feat = extract_features(test)\n# test_feat = extract_url_license_feat(test_feat)\n# test_feat = handle_cate_NA(test_feat)\n# test_feat = handle_cont_NA(test_feat)\n# test_feat.head()\n\ntest_feat = pd.read_csv('../input/commonlitfe/test_feat.csv', low_memory=False)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T21:50:37.241466Z","iopub.execute_input":"2021-05-25T21:50:37.241862Z","iopub.status.idle":"2021-05-25T21:50:37.279857Z","shell.execute_reply.started":"2021-05-25T21:50:37.241823Z","shell.execute_reply":"2021-05-25T21:50:37.278629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca_groups = [['smog', 'syll_per_word', 'spacy_29'], ['coleman_liau', 'nominalization', 'IN'], ['spacy_68', 'spacy_86', 'spacy_208', 'spacy_262', 'spacy_147', 'spacy_261'], ['spacy_110', 'spacy_114', 'spacy_298', 'spacy_269', 'spacy_151'], ['spacy_76', 'spacy_122', 'periods', 'spacy_72', 'spacy_196'], ['spacy_4', 'spacy_214', 'spacy_101', 'flesch', 'periods'], ['pronoun', 'spacy_269', 'spacy_294', 'spacy_151', 'spacy_147', 'spacy_110', 'spacy_196'], ['spacy_264', 'spacy_134', 'spacy_122', 'spacy_86', 'spacy_254', 'spacy_72'], ['spacy_76', 'spacy_114', 'spacy_298', 'spacy_69'], ['spacy_28', 'spacy_269', 'spacy_151', 'spacy_122', 'spacy_72', 'spacy_69', 'spacy_134', 'spacy_9', 'spacy_254'], ['spacy_101', 'spacy_214', 'spacy_262', 'spacy_89', 'spacy_110', 'spacy_208'], ['spacy_86', 'spacy_105', 'spacy_249', 'spacy_294', 'VBD', 'spacy_147', 'flesch', 'periods'], ['spacy_28', 'pronoun', 'spacy_122', 'spacy_101', 'spacy_110', 'periods', 'spacy_9'], ['spacy_249', 'PRP', 'spacy_68', 'spacy_294', 'VBD', 'spacy_261', 'spacy_4', 'spacy_298'], ['spacy_76', 'spacy_72', 'spacy_208', 'spacy_89', 'flesch', 'spacy_196', 'spacy_69'], ['spacy_249', 'spacy_68', 'spacy_76', 'spacy_122', 'spacy_208', 'spacy_214', 'spacy_101', 'spacy_254'], ['PRP', 'spacy_114', 'spacy_28', 'spacy_151', 'spacy_4', 'spacy_110', 'spacy_279', 'spacy_232'], ['VBD', 'spacy_264', 'spacy_134', 'spacy_269', 'spacy_261'], ['spacy_249', 'spacy_110', 'spacy_68', 'spacy_9', 'spacy_122', 'spacy_28', 'spacy_147'], ['spacy_269', 'PRP', 'spacy_151', 'spacy_298', 'spacy_101', 'spacy_198', 'spacy_72'], ['spacy_198', 'spacy_28', 'spacy_151', 'spacy_269', 'spacy_261'], ['spacy_249', 'PRP', 'spacy_114', 'spacy_122', 'spacy_110', 'spacy_264', 'spacy_208', 'spacy_133'], ['spacy_214', 'PRP', 'spacy_114', 'spacy_86', 'periods', 'spacy_28'], ['pronoun', 'spacy_76', 'spacy_68', 'spacy_262', 'VBD', 'spacy_122', 'spacy_105', 'spacy_298'], ['spacy_151', 'spacy_134', 'spacy_269', 'spacy_279', 'periods', 'spacy_89', 'spacy_133', 'spacy_147', 'spacy_232'], ['spacy_249', 'spacy_86', 'PRP', 'spacy_114', 'spacy_122', 'spacy_69', 'spacy_294', 'spacy_68', 'spacy_254', 'spacy_110'], ['PRP', 'spacy_214', 'pronoun', 'VBD', 'spacy_114', 'spacy_254', 'spacy_294', 'spacy_261', 'spacy_208', 'spacy_134', 'spacy_4', 'spacy_89', 'spacy_298'], ['spacy_269', 'spacy_249', 'spacy_151', 'spacy_76', 'spacy_122', 'spacy_101', 'periods'], ['spacy_294', 'spacy_214', 'spacy_76', 'spacy_28', 'spacy_86', 'spacy_264', 'spacy_232', 'spacy_122', 'spacy_9'], ['spacy_294', 'spacy_68', 'spacy_122', 'spacy_4', 'spacy_264', 'spacy_261', 'spacy_196', 'spacy_9'], ['spacy_114', 'spacy_249', 'spacy_86', 'spacy_151', 'spacy_134', 'spacy_101', 'spacy_76', 'spacy_254', 'spacy_262'], ['spacy_249', 'PRP', 'pronoun', 'spacy_28', 'spacy_269', 'spacy_114', 'spacy_68', 'spacy_294', 'spacy_122', 'spacy_261', 'spacy_196', 'spacy_72', 'spacy_133'], ['spacy_86', 'spacy_76', 'flesch', 'spacy_4', 'spacy_89', 'spacy_110', 'spacy_9', 'spacy_151'], ['spacy_110', 'spacy_86', 'spacy_208', 'spacy_214', 'spacy_134'], ['spacy_114', 'spacy_269', 'spacy_249', 'spacy_101', 'spacy_76', 'spacy_198', 'spacy_9', 'flesch'], ['spacy_122', 'spacy_294', 'spacy_72'], ['spacy_298', 'spacy_214', 'VBD', 'spacy_114', 'spacy_264', 'spacy_68', 'spacy_9', 'spacy_134', 'spacy_262', 'spacy_4', 'spacy_147', 'flesch'], ['spacy_249', 'spacy_69', 'spacy_105', 'spacy_89', 'spacy_110'], ['preposition', 'smog'], ['spacy_14', 'chars_per_word', 'spacy_29'], ['IN', 'spacy_107'], ['spacy_160', 'nominalization', 'spacy_2'], ['syll_per_word', 'NN', 'spacy_200'], ['coleman_liau', 'NN', 'spacy_60', 'spacy_263'], ['spacy_14', 'dale_chall', 'JJ', 'spacy_182'], ['nominalization', 'spacy_46', 'spacy_155', 'spacy_107'], ['spacy_203', 'smog', 'spacy_149', 'spacy_24'], ['spacy_103', 'avg_len_word', 'num_char', 'spacy_200'], ['spacy_146', 'rix'], ['syll_per_word', 'IN', 'spacy_159', 'spacy_10'], ['preposition', 'spacy_29', 'syll_per_word', 'IN', 'spacy_10'], ['IN', 'spacy_29', 'num_char', 'spacy_182', 'spacy_240', 'spacy_10'], ['chars_per_word', 'avg_len_word', 'nominalization', 'rix', 'spacy_149', 'JJ', 'spacy_2'], ['smog', 'spacy_60', 'spacy_146', 'spacy_97', 'spacy_162'], ['spacy_50', 'spacy_14', 'spacy_38', 'spacy_192', 'dale_chall', 'spacy_24'], ['spacy_203', 'coleman_liau', 'spacy_107', 'preposition'], ['spacy_14', 'coleman_liau', 'spacy_43', 'spacy_182', 'spacy_258'], ['smog', 'rix', 'dale_chall', 'spacy_197', 'spacy_251', 'spacy_107', 'spacy_240'], ['spacy_160', 'IN', 'spacy_27', 'spacy_192', 'JJ', 'nominalization'], ['chars_per_word', 'spacy_7', 'spacy_148', 'spacy_97', 'spacy_159', 'spacy_217', 'lix', 'spacy_200'], ['spacy_155', 'spacy_203', 'avg_len_word', 'spacy_60', 'spacy_252', 'NN', 'spacy_146', 'gunning_fog'], ['syll_per_word', 'num_char', 'spacy_38', 'spacy_24', 'preposition'], ['coleman_liau', 'spacy_2', 'spacy_160', 'spacy_107', 'spacy_162'], ['smog', 'spacy_43', 'spacy_197', 'dale_chall', 'spacy_155'], ['spacy_29', 'IN', 'chars_per_word', 'spacy_252', 'spacy_24', 'preposition', 'lix', 'spacy_200', 'spacy_50'], ['spacy_14', 'syll_per_word', 'spacy_7', 'spacy_27', 'nominalization', 'spacy_211', 'gunning_fog', 'spacy_10'], ['avg_len_word', 'spacy_251', 'rix', 'spacy_30', 'spacy_217', 'spacy_149'], ['spacy_97', 'spacy_149', 'spacy_200', 'spacy_263', 'spacy_162', 'spacy_182', 'gunning_fog'], ['spacy_14', 'spacy_203', 'spacy_43', 'JJ', 'spacy_30', 'dale_chall', 'spacy_146', 'spacy_50', 'spacy_10'], ['coleman_liau', 'spacy_251', 'nominalization', 'spacy_7', 'spacy_46', 'spacy_155', 'preposition', 'spacy_240', 'spacy_266', 'lix'], ['chars_per_word', 'syll_per_word', 'smog', 'IN', 'spacy_38', 'spacy_107', 'spacy_2'], ['nominalization', 'spacy_251', 'IN', 'spacy_155', 'NN', 'spacy_24', 'gunning_fog'], ['smog', 'coleman_liau', 'spacy_14', 'spacy_146', 'spacy_266', 'spacy_240', 'spacy_182'], ['syll_per_word', 'spacy_43', 'spacy_197', 'spacy_46', 'spacy_217', 'spacy_38', 'dale_chall', 'spacy_192', 'rix', 'spacy_200', 'spacy_162'], ['num_char', 'spacy_29', 'lix', 'spacy_10'], ['avg_len_word', 'spacy_10', 'preposition'], ['spacy_217', 'spacy_149', 'spacy_97', 'spacy_24', 'num_char', 'syll_per_word', 'spacy_266'], ['spacy_252', 'spacy_29', 'spacy_14', 'spacy_30', 'spacy_211', 'spacy_192', 'dale_chall', 'spacy_107', 'spacy_60', 'lix'], ['coleman_liau', 'smog', 'spacy_197', 'spacy_251', 'rix', 'spacy_27'], ['spacy_103', 'spacy_160', 'chars_per_word', 'spacy_7', 'spacy_146', 'spacy_240'], ['chars_per_word', 'spacy_29', 'spacy_30', 'spacy_217', 'spacy_197', 'spacy_155', 'spacy_24', 'spacy_97', 'spacy_162', 'spacy_182', 'spacy_10'], ['coleman_liau', 'avg_len_word', 'smog', 'spacy_258', 'spacy_7', 'preposition', 'JJ', 'spacy_240'], ['IN', 'num_char', 'spacy_38', 'lix'], ['spacy_29', 'spacy_2', 'dale_chall', 'spacy_203', 'spacy_263', 'spacy_107', 'spacy_160'], ['coleman_liau', 'syll_per_word', 'avg_len_word', 'nominalization', 'spacy_103', 'num_char', 'spacy_38', 'spacy_192', 'JJ'], ['chars_per_word', 'spacy_252', 'spacy_14', 'spacy_197', 'spacy_211', 'spacy_182', 'spacy_240', 'lix', 'gunning_fog', 'spacy_266'], ['chars_per_word', 'spacy_27', 'spacy_149', 'spacy_197', 'NN', 'preposition', 'spacy_97', 'gunning_fog'], ['nominalization', 'rix', 'spacy_14', 'num_char'], ['coleman_liau', 'syll_per_word', 'avg_len_word', 'smog', 'spacy_103', 'JJ', 'spacy_251', 'dale_chall'], ['spacy_43', 'spacy_2', 'spacy_103', 'spacy_192', 'spacy_240', 'spacy_200', 'spacy_146', 'spacy_149', 'spacy_24', 'spacy_107', 'spacy_263'], ['coleman_liau', 'avg_len_word', 'nominalization', 'smog', 'spacy_7', 'rix', 'spacy_46', 'spacy_29', 'spacy_30', 'spacy_162', 'lix', 'gunning_fog', 'spacy_10'], ['spacy_251', 'spacy_38', 'spacy_60', 'dale_chall', 'num_char', 'spacy_252', 'spacy_266', 'JJ', 'spacy_217', 'spacy_24', 'spacy_182'], ['nominalization', 'spacy_97', 'spacy_148', 'spacy_240'], ['spacy_7', 'spacy_29', 'spacy_14', 'spacy_43', 'spacy_2'], ['coleman_liau', 'chars_per_word', 'syll_per_word', 'rix', 'spacy_146', 'spacy_197', 'spacy_192'], ['spacy_148', 'spacy_258', 'spacy_7', 'spacy_149', 'rix', 'spacy_107', 'dale_chall'], ['syll_per_word', 'avg_len_word', 'smog', 'nominalization', 'spacy_263', 'spacy_203', 'spacy_211', 'preposition', 'spacy_200', 'spacy_162'], ['spacy_251', 'spacy_46', 'spacy_43', 'IN', 'spacy_155', 'gunning_fog'], ['spacy_29', 'spacy_30', 'spacy_38', 'JJ', 'spacy_107', 'spacy_162', 'lix', 'gunning_fog'], ['coleman_liau', 'spacy_46', 'spacy_252', 'spacy_14', 'spacy_251', 'spacy_200', 'nominalization', 'rix', 'num_char', 'spacy_155'], ['syll_per_word', 'chars_per_word', 'spacy_146', 'spacy_217', 'spacy_159', 'spacy_266'], ['chars_per_word', 'syll_per_word', 'IN', 'spacy_146', 'smog'], ['rix', 'spacy_14', 'spacy_263'], ['coleman_liau', 'avg_len_word', 'spacy_43', 'nominalization', 'spacy_203', 'dale_chall'], ['spacy_203', 'spacy_2', 'dale_chall', 'spacy_217', 'spacy_266', 'preposition', 'spacy_159', 'spacy_162', 'lix'], ['num_char', 'spacy_149', 'JJ', 'spacy_46', 'gunning_fog'], ['syll_per_word', 'coleman_liau', 'chars_per_word', 'smog', 'spacy_43', 'nominalization', 'num_char', 'dale_chall', 'spacy_197', 'spacy_192', 'spacy_149', 'spacy_97', 'spacy_146', 'lix'], ['spacy_27', 'rix', 'spacy_103', 'IN', 'spacy_162', 'NN', 'gunning_fog', 'spacy_10'], ['spacy_107', 'preposition', 'spacy_266'], ['chars_per_word', 'avg_len_word', 'smog', 'spacy_103', 'nominalization', 'spacy_46', 'spacy_14'], ['spacy_155', 'spacy_97', 'spacy_27', 'spacy_107', 'spacy_182', 'lix', 'gunning_fog', 'spacy_266'], ['coleman_liau', 'chars_per_word', 'nominalization', 'rix', 'smog', 'spacy_263', 'spacy_2'], ['syll_per_word', 'avg_len_word', 'spacy_103', 'spacy_149'], ['spacy_27', 'dale_chall', 'spacy_211', 'NN', 'spacy_24'], ['spacy_155', 'spacy_27', 'dale_chall', 'spacy_211', 'NN', 'avg_len_word', 'spacy_162', 'spacy_24'], ['spacy_14', 'avg_len_word', 'spacy_160', 'spacy_29', 'spacy_197', 'spacy_30', 'spacy_155', 'spacy_27', 'dale_chall']]\n\ndrop_other_fe = ['periods', 'commas', 'semis', 'exclaims', 'questions', 'num_char', 'num_words', 'unique_words', 'word_diversity', 'longest_word', 'avg_len_word']\n\nfor index, group in enumerate(pca_groups):\n    key = 'f'+str(index)\n    pca_res, train_feat = train_pca(train_feat, group, key)\n    test_feat = apply_pca(pca_res, test_feat)\n\ntrain_feat.drop(columns = drop_other_fe, inplace=True, axis = 1)\ntest_feat.drop(columns = drop_other_fe, inplace=True, axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T21:50:37.281802Z","iopub.execute_input":"2021-05-25T21:50:37.282154Z","iopub.status.idle":"2021-05-25T21:50:46.21349Z","shell.execute_reply.started":"2021-05-25T21:50:37.282112Z","shell.execute_reply":"2021-05-25T21:50:46.212189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"std_error = copy.deepcopy(train_feat['standard_error'])\ntrain_feat.drop(columns=['standard_error'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T21:50:46.216467Z","iopub.execute_input":"2021-05-25T21:50:46.216929Z","iopub.status.idle":"2021-05-25T21:50:46.233008Z","shell.execute_reply.started":"2021-05-25T21:50:46.216887Z","shell.execute_reply":"2021-05-25T21:50:46.23129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nignore_cols = ['id','url_legal','license','excerpt', 'target']\n\nfor col in train_feat.select_dtypes('object').columns.tolist():\n    if col not in ignore_cols:\n        lbl = LabelEncoder()\n        train_feat[col] = lbl.fit_transform(train_feat[col])\n        test_feat[col] = lbl.transform(test_feat[col])","metadata":{"execution":{"iopub.status.busy":"2021-05-25T21:50:46.235193Z","iopub.execute_input":"2021-05-25T21:50:46.235695Z","iopub.status.idle":"2021-05-25T21:50:46.298195Z","shell.execute_reply.started":"2021-05-25T21:50:46.235644Z","shell.execute_reply":"2021-05-25T21:50:46.29691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = train_feat[[i for i in train_feat.columns if i not in ignore_cols]]\ny_train = train_feat['target']\ntest_X = test_feat[[i for i in test_feat.columns if i not in ignore_cols]]\n[i for i in train_feat.columns if i not in test_feat.columns]","metadata":{"execution":{"iopub.status.busy":"2021-05-25T21:50:46.29992Z","iopub.execute_input":"2021-05-25T21:50:46.300458Z","iopub.status.idle":"2021-05-25T21:50:46.323508Z","shell.execute_reply.started":"2021-05-25T21:50:46.300401Z","shell.execute_reply":"2021-05-25T21:50:46.322239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape)\nprint(test_X.shape)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T21:50:46.325114Z","iopub.execute_input":"2021-05-25T21:50:46.325507Z","iopub.status.idle":"2021-05-25T21:50:46.331272Z","shell.execute_reply.started":"2021-05-25T21:50:46.32547Z","shell.execute_reply":"2021-05-25T21:50:46.33043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics, preprocessing, model_selection\nimport lightgbm as lgb\n\ndef runLGB_reg(train_X, train_y, test_X, sample_weight, test_y=None, test_X2=None, dep=8, seed=0, data_leaf=50, rounds=20000):\n    params = {}\n    params[\"objective\"] = \"regression\"\n    params['metric'] = 'rmse'\n    params[\"max_depth\"] = dep\n    params[\"num_leaves\"] = 30\n    params[\"min_data_in_leaf\"] = data_leaf\n    #     params[\"min_sum_hessian_in_leaf\"] = 50\n    params[\"learning_rate\"] = 0.01\n    params[\"bagging_fraction\"] = 0.8\n    params[\"feature_fraction\"] = 0.2\n    params[\"feature_fraction_seed\"] = seed\n    params[\"bagging_freq\"] = 1\n    params[\"bagging_seed\"] = seed\n    params[\"lambda_l2\"] = 3\n    params[\"lambda_l1\"] = 3\n    params[\"verbosity\"] = -1\n    num_rounds = rounds\n\n    plst = list(params.items())\n    lgtrain = lgb.Dataset(train_X, label=train_y)\n\n    if test_y is not None:\n        lgtest = lgb.Dataset(test_X, label=test_y)\n        model = lgb.train(params, lgtrain, num_rounds, valid_sets=[lgtest], early_stopping_rounds=200, verbose_eval=500)\n    else:\n        lgtest = lgb.DMatrix(test_X)\n        model = lgb.train(params, lgtrain, num_rounds)\n\n    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n    pred_test_y2 = model.predict(test_X2, num_iteration=model.best_iteration)\n\n    loss = 0\n    if test_y is not None:\n        loss = np.sqrt(metrics.mean_squared_error(test_y, pred_test_y))\n        print(loss)\n        return model, loss, pred_test_y, pred_test_y2\n    else:\n        return model, loss, pred_test_y, pred_test_y2","metadata":{"execution":{"iopub.status.busy":"2021-05-25T21:50:46.332625Z","iopub.execute_input":"2021-05-25T21:50:46.333093Z","iopub.status.idle":"2021-05-25T21:50:46.346599Z","shell.execute_reply.started":"2021-05-25T21:50:46.333036Z","shell.execute_reply":"2021-05-25T21:50:46.345489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Building model..\")\ncv_scores = []\npred_test_full = 0\npred_train = np.zeros(X_train.shape[0])\nn_splits = 5\nkf = model_selection.KFold(n_splits=n_splits, shuffle=True, random_state=7988)\nmodel_name = \"lgb\"\nfor dev_index, val_index in kf.split(X_train, y_train):\n    dev_X, val_X = X_train.iloc[dev_index,:], X_train.iloc[val_index,:]\n    dev_y, val_y = y_train[dev_index], y_train[val_index]\n    std_error_x = std_error[dev_index]\n\n    pred_val = 0\n    pred_test = 0\n    n_models = 0.\n\n    model, loss, pred_v, pred_t = runLGB_reg(dev_X, dev_y, val_X, std_error_x, val_y, test_X, dep=6, data_leaf=200, seed=2019)\n    pred_val += pred_v\n    pred_test += pred_t\n    n_models += 1\n    \n    model, loss, pred_v, pred_t = runLGB_reg(dev_X, dev_y, val_X, std_error_x, val_y, test_X,  dep=7, data_leaf=180, seed=9873)\n    pred_val += pred_v\n    pred_test += pred_t\n    n_models += 1\n\n    pred_val /= n_models\n    pred_test /= n_models\n    \n    loss = np.sqrt(metrics.mean_squared_error(val_y, pred_val))\n        \n    pred_train[val_index] = pred_val\n    pred_test_full += pred_test / n_splits\n    cv_scores.append(loss)\n    print(cv_scores)\nprint(np.mean(cv_scores))","metadata":{"execution":{"iopub.status.busy":"2021-05-25T21:50:46.348317Z","iopub.execute_input":"2021-05-25T21:50:46.349011Z","iopub.status.idle":"2021-05-25T21:52:16.475511Z","shell.execute_reply.started":"2021-05-25T21:50:46.348955Z","shell.execute_reply":"2021-05-25T21:52:16.474422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}