{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport random\nimport copy\n\nfrom transformers import (AutoConfig, AutoModel, AutoTokenizer, AdamW, \n                          get_linear_schedule_with_warmup, logging, \n                          RobertaConfig, PreTrainedModel, RobertaModel)\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, SequentialSampler, RandomSampler, DataLoader\n\nfrom tqdm.notebook import tqdm\n\nimport gc; gc.enable()\nfrom IPython.display import clear_output\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.metrics import f1_score\n\nlogging.set_verbosity_error()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-29T02:44:06.72464Z","iopub.execute_input":"2021-07-29T02:44:06.725088Z","iopub.status.idle":"2021-07-29T02:44:14.091216Z","shell.execute_reply.started":"2021-07-29T02:44:06.724947Z","shell.execute_reply":"2021-07-29T02:44:14.089565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"INPUT_DIR = '../input/commonlitreadabilityprize'\nMODEL_NAME = 'roberta-large'\n\nMAX_LENGTH = 256\nLR = 2e-5\nEPS = 1e-8\n\nSEED = 42\n\nNUM_FOLDS = 5\nSEEDS = [113, 71, 17, 43, 37]\n\nEPOCHS = 5\nTRAIN_BATCH_SIZE = 8\nVAL_BATCH_SIZE = 32\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2021-07-29T02:44:14.092734Z","iopub.execute_input":"2021-07-29T02:44:14.093077Z","iopub.status.idle":"2021-07-29T02:44:14.156345Z","shell.execute_reply.started":"2021-07-29T02:44:14.093031Z","shell.execute_reply":"2021-07-29T02:44:14.155263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed = 0):\n    np.random.seed(seed)\n    random_state = np.random.RandomState(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    return random_state\n\nseed=1112\nrandom_state = set_seed(seed)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T02:44:14.161791Z","iopub.execute_input":"2021-07-29T02:44:14.163998Z","iopub.status.idle":"2021-07-29T02:44:14.177918Z","shell.execute_reply.started":"2021-07-29T02:44:14.163957Z","shell.execute_reply":"2021-07-29T02:44:14.176893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_data_loaders(data, fold):\n    \n    x_train = data.loc[data.fold != fold, 'excerpt'].tolist()\n    y_train_labels = data.loc[data.fold != fold, 'coded_target'].values\n    x_val = data.loc[data.fold == fold, 'excerpt'].tolist()\n    y_val_labels = data.loc[data.fold == fold, 'coded_target'].values\n    \n    y_train = [[float(i) for i in x] for x in y_train_labels]\n    y_val = [[float(i) for i in x] for x in y_val_labels]\n    \n    encoded_train = tokenizer.batch_encode_plus(\n        x_train, \n        add_special_tokens=True, \n        return_attention_mask=True, \n        padding='max_length', \n        truncation=True,\n        max_length=MAX_LENGTH, \n        return_tensors='pt'\n    )\n    \n    encoded_val = tokenizer.batch_encode_plus(\n        x_val, \n        add_special_tokens=True, \n        return_attention_mask=True, \n        padding='max_length', \n        truncation=True,\n        max_length=MAX_LENGTH, \n        return_tensors='pt'\n    )\n    \n    dataset_train = TensorDataset(\n        encoded_train['input_ids'],\n        encoded_train['attention_mask'],\n        torch.tensor(y_train)\n    )\n    dataset_val = TensorDataset(\n        encoded_val['input_ids'],\n        encoded_val['attention_mask'],\n        torch.tensor(y_val)\n    )\n    \n    dataloader_train = DataLoader(\n        dataset_train,\n        sampler = RandomSampler(dataset_train),\n        batch_size=TRAIN_BATCH_SIZE\n    )\n\n    dataloader_val = DataLoader(\n        dataset_val,\n        sampler = SequentialSampler(dataset_val),\n        batch_size=VAL_BATCH_SIZE\n    )\n\n    return dataloader_train, dataloader_val","metadata":{"execution":{"iopub.status.busy":"2021-07-29T02:44:14.182305Z","iopub.execute_input":"2021-07-29T02:44:14.184375Z","iopub.status.idle":"2021-07-29T02:44:14.199131Z","shell.execute_reply.started":"2021-07-29T02:44:14.184304Z","shell.execute_reply":"2021-07-29T02:44:14.197935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RobertaPreTrainedModel(PreTrainedModel):\n    config_class = RobertaConfig\n    base_model_prefix = \"roberta\"\n\n    def _init_weights(self, module):\n        \"\"\"Initialize the weights\"\"\"\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n    def update_keys_to_ignore(self, config, del_keys_to_ignore):\n        \"\"\"Remove some keys from ignore list\"\"\"\n        if not config.tie_word_embeddings:\n            self._keys_to_ignore_on_save = [k for k in self._keys_to_ignore_on_save if k not in del_keys_to_ignore]\n            self._keys_to_ignore_on_load_missing = [\n                k for k in self._keys_to_ignore_on_load_missing if k not in del_keys_to_ignore\n            ]\n\nclass RobertaForMultiClass(RobertaPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.config = config\n\n        self.roberta = RobertaModel(config, add_pooling_layer=False)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.relu = nn.ReLU()\n        self.out_proj = nn.Linear(config.hidden_size, 3)\n        self.loss = nn.BCEWithLogitsLoss()\n        \n        self.init_weights()\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.roberta(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        last_hidden_state = outputs[0]\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        \n        mean_embeddings = self.dropout(mean_embeddings)\n        mean_embeddings = self.dense(mean_embeddings)\n        mean_embeddings = self.relu(mean_embeddings)\n        mean_embeddings = self.dropout(mean_embeddings)\n        logits = self.out_proj(mean_embeddings)\n\n        if labels is not None:\n            loss = self.loss(logits, labels)\n            return loss\n        else:\n            return logits","metadata":{"execution":{"iopub.status.busy":"2021-07-29T02:44:14.203314Z","iopub.execute_input":"2021-07-29T02:44:14.206402Z","iopub.status.idle":"2021-07-29T02:44:14.232456Z","shell.execute_reply.started":"2021-07-29T02:44:14.206364Z","shell.execute_reply":"2021-07-29T02:44:14.231693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_segments(total_bins, bin_len):\n    bins = [(0, bin_len)]\n    for seg in range(total_bins-1):\n        if seg == total_bins-2:\n            bins.append((bins[-1][1], np.nan))\n        else:\n            bins.append((bins[-1][1], bins[-1][1]+bin_len))\n    return bins\n\ndef create_bins(df, column, num_of_bins=5):\n    bin_len = int(len(df)/num_of_bins)\n    bins = create_segments(num_of_bins, bin_len)\n    df = df.sort_values(column, ascending=True).reset_index(drop=True)\n    column_name = column+\"_segment\"\n    df[column_name] = 0\n    for index, seg in enumerate(bins):\n        start = seg[0]\n        end = seg[1]\n        if end is not np.nan:\n            df.loc[start:end, column_name] = str(index)\n        else:\n            df.loc[start:, column_name] = str(index)\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-07-29T02:44:14.236426Z","iopub.execute_input":"2021-07-29T02:44:14.239088Z","iopub.status.idle":"2021-07-29T02:44:14.251578Z","shell.execute_reply.started":"2021-07-29T02:44:14.239021Z","shell.execute_reply":"2021-07-29T02:44:14.250675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(os.path.join(INPUT_DIR, 'train.csv'))\n\nbins = 4\ndata = create_bins(copy.deepcopy(data), \"standard_error\", bins)\n\nbi_modal = data.loc[data['standard_error_segment'] == str(3)].sort_values(by='target').reset_index(drop=True)\nG_keys = data.loc[(data['standard_error_segment'] == '0') | (data['standard_error_segment'] == '1') | (data['standard_error_segment'] == '2'), 'target'].values\nB0_keys = bi_modal[:355]['target'].values\nB1_keys = bi_modal[355:]['target'].values\n\ndef assign_class_labels(row):\n    if row['target'] in G_keys:\n        return 'A'\n    elif row['target'] in B0_keys:\n        return 'B'\n    else: return 'C'\n\ndata['class_labels'] = data.apply(lambda row: assign_class_labels(row), axis=1)\n\nlb = LabelBinarizer()\nlb = lb.fit(data.class_labels.values)\ndata['coded_target'] = lb.transform(data.class_labels.values).tolist()\n\n# Create stratified folds\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\nfor f, (t_, v_) in enumerate(kf.split(data, data.class_labels)):\n    data.loc[v_, 'fold'] = f\ndata['fold'] = data['fold'].astype(int)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T02:44:14.255623Z","iopub.execute_input":"2021-07-29T02:44:14.258202Z","iopub.status.idle":"2021-07-29T02:44:14.470461Z","shell.execute_reply.started":"2021-07-29T02:44:14.258165Z","shell.execute_reply":"2021-07-29T02:44:14.469569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, val_dataloader):\n    model.eval()\n    loss_val_total = 0\n    for batch in val_dataloader:\n        batch = tuple(b.to(DEVICE) for b in batch)\n        inputs = {'input_ids':      batch[0],\n                  'attention_mask': batch[1],\n                  'labels':         batch[2],\n                 }\n        with torch.no_grad():        \n            loss = model(**inputs)\n        loss_val_total += loss.item()\n    loss_val_avg = loss_val_total/len(val_dataloader) \n    return loss_val_avg\n\ndef train(model, train_dataloader, val_dataloader):\n    optimizer = AdamW(model.parameters(), lr = LR, eps = EPS)\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * EPOCHS)\n    best_val_loss = 1\n    model.train()\n    for epoch in range(EPOCHS):\n        loss_train_total = 0\n        for batch in tqdm(train_dataloader):\n            model.zero_grad()\n            batch = tuple(b.to(DEVICE) for b in batch)\n            inputs = {\n                'input_ids': batch[0],\n                'attention_mask': batch[1],\n                'labels': batch[2]\n            }\n            loss = model(**inputs)\n            loss_train_total += loss.item()\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n        loss_train_avg = loss_train_total / len(train_dataloader)\n        loss_val_avg = evaluate(model, val_dataloader)\n        print(f'epoch:{epoch+1}/{EPOCHS} train loss={loss_train_avg}  val loss={loss_val_avg}')\n        \n        if loss_val_avg < best_val_loss:\n            best_val_loss = loss_val_avg    \n    return best_val_loss","metadata":{"execution":{"iopub.status.busy":"2021-07-29T02:44:14.472552Z","iopub.execute_input":"2021-07-29T02:44:14.472932Z","iopub.status.idle":"2021-07-29T02:44:14.483373Z","shell.execute_reply.started":"2021-07-29T02:44:14.472899Z","shell.execute_reply":"2021-07-29T02:44:14.482435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"losses = []\n\nMAX_RUNS = 2\nruns = 0   # Variable to control termination condition\n\nmodel = RobertaForMultiClass.from_pretrained(MODEL_NAME)\nmodel.to(DEVICE)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\nfor i, seed in enumerate(SEEDS):       \n    # Termination condition\n    if runs == MAX_RUNS:\n        print(f'{runs} runs termination condition reached.')\n        break    \n    \n    print(f'********* seed({i}) = {seed} ***********')\n    \n    for fold in range(NUM_FOLDS):\n        print(f'*** fold = {fold} ***')\n        set_seed(seed)\n        train_dataloader, val_dataloader = get_data_loaders(data, fold)\n            \n        loss = train(model, train_dataloader, val_dataloader)\n        losses.append(loss)\n        \n        # Termination condition\n        runs += 1\n        if runs == MAX_RUNS:\n            break","metadata":{"execution":{"iopub.status.busy":"2021-07-29T02:44:14.485122Z","iopub.execute_input":"2021-07-29T02:44:14.485467Z","iopub.status.idle":"2021-07-29T03:22:45.514997Z","shell.execute_reply.started":"2021-07-29T02:44:14.485435Z","shell.execute_reply":"2021-07-29T03:22:45.513517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataloader, val_dataloader = get_data_loaders(data, 4)\n\nmodel.eval()\npredictions = []\nlabels = []\nwith torch.no_grad():\n    for batch in val_dataloader:\n        batch = tuple(b.to(DEVICE) for b in batch)\n        inputs = {\n            'input_ids': batch[0],\n            'attention_mask': batch[1],\n        }\n        labels.extend(batch[2].cpu().detach().numpy())\n        outputs = model(**inputs)\n        predictions.extend(outputs.cpu().detach().numpy())\ny_pred = np.argmax(predictions, axis=-1)\ny_true = np.asarray([0 if v == 'A' else 1 if v == 'B' else 2 for v in lb.inverse_transform(np.asarray([[int(i) for i in x] for x in labels]))]).reshape(-1)\n\nf1_score(y_true, y_pred, average='micro')","metadata":{"execution":{"iopub.status.busy":"2021-07-29T03:22:45.517001Z","iopub.execute_input":"2021-07-29T03:22:45.51736Z","iopub.status.idle":"2021-07-29T03:23:01.498614Z","shell.execute_reply.started":"2021-07-29T03:22:45.517322Z","shell.execute_reply":"2021-07-29T03:23:01.497577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save_pretrained(\"/kaggle/working\")","metadata":{"execution":{"iopub.status.busy":"2021-07-29T03:23:01.500046Z","iopub.execute_input":"2021-07-29T03:23:01.500482Z","iopub.status.idle":"2021-07-29T03:23:05.746951Z","shell.execute_reply.started":"2021-07-29T03:23:01.50044Z","shell.execute_reply":"2021-07-29T03:23:05.745896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}