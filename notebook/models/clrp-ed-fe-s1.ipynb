{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### Thanks to Abhishek Thakur for the AutoNLP trained Language Models. In this kernel, we will be using these models and taking out the last layer embedding which would go as features for the machine learning model.","metadata":{}},{"cell_type":"code","source":"from requests.adapters import BaseAdapter\nfrom requests.compat import urlparse, unquote\nfrom requests import Response, codes\nimport errno\nimport os\nimport stat\nimport locale\nimport io\n\nfrom six import BytesIO\n\n\nclass FileAdapter(BaseAdapter):\n    def __init__(self, set_content_length=True):\n        super(FileAdapter, self).__init__()\n        self._set_content_length = set_content_length\n\n    def send(self, request, **kwargs):\n        \"\"\"Wraps a file, described in request, in a Response object.\n        :param request: The PreparedRequest` being \"sent\".\n        :returns: a Response object containing the file\n        \"\"\"\n\n        # Check that the method makes sense. Only support GET\n        if request.method not in (\"GET\", \"HEAD\"):\n            raise ValueError(\"Invalid request method %s\" % request.method)\n\n        # Parse the URL\n        url_parts = urlparse(request.url)\n\n        # Reject URLs with a hostname component\n        if url_parts.netloc and url_parts.netloc != \"localhost\":\n            raise ValueError(\"file: URLs with hostname components are not permitted\")\n\n        resp = Response()\n\n        # Open the file, translate certain errors into HTTP responses\n        # Use urllib's unquote to translate percent escapes into whatever\n        # they actually need to be\n        try:\n            # Split the path on / (the URL directory separator) and decode any\n            # % escapes in the parts\n            path_parts = [unquote(p) for p in url_parts.path.split(\"/\")]\n\n            # Strip out the leading empty parts created from the leading /'s\n            while path_parts and not path_parts[0]:\n                path_parts.pop(0)\n\n            # If os.sep is in any of the parts, someone fed us some shenanigans.\n            # Treat is like a missing file.\n            if any(os.sep in p for p in path_parts):\n                raise IOError(errno.ENOENT, os.strerror(errno.ENOENT))\n\n            # Look for a drive component. If one is present, store it separately\n            # so that a directory separator can correctly be added to the real\n            # path, and remove any empty path parts between the drive and the path.\n            # Assume that a part ending with : or | (legacy) is a drive.\n            if path_parts and (\n                path_parts[0].endswith(\"|\") or path_parts[0].endswith(\":\")\n            ):\n                path_drive = path_parts.pop(0)\n                if path_drive.endswith(\"|\"):\n                    path_drive = path_drive[:-1] + \":\"\n\n                while path_parts and not path_parts[0]:\n                    path_parts.pop(0)\n            else:\n                path_drive = \"\"\n\n            # Try to put the path back together\n            # Join the drive back in, and stick os.sep in front of the path to\n            # make it absolute.\n            path = path_drive + os.sep + os.path.join(*path_parts)\n\n            # Check if the drive assumptions above were correct. If path_drive\n            # is set, and os.path.splitdrive does not return a drive, it wasn't\n            # really a drive. Put the path together again treating path_drive\n            # as a normal path component.\n            if path_drive and not os.path.splitdrive(path):\n                path = os.sep + os.path.join(path_drive, *path_parts)\n\n            # Use io.open since we need to add a release_conn method, and\n            # methods can't be added to file objects in python 2.\n            resp.raw = io.open(path, \"rb\")\n            resp.raw.release_conn = resp.raw.close\n        except IOError as e:\n            if e.errno == errno.EACCES:\n                resp.status_code = codes.forbidden\n            elif e.errno == errno.ENOENT:\n                resp.status_code = codes.not_found\n            else:\n                resp.status_code = codes.bad_request\n\n            # Wrap the error message in a file-like object\n            # The error message will be localized, try to convert the string\n            # representation of the exception into a byte stream\n            resp_str = str(e).encode(locale.getpreferredencoding(False))\n            resp.raw = BytesIO(resp_str)\n            if self._set_content_length:\n                resp.headers[\"Content-Length\"] = len(resp_str)\n\n            # Add release_conn to the BytesIO object\n            resp.raw.release_conn = resp.raw.close\n        else:\n            resp.status_code = codes.ok\n            resp.url = request.url\n\n            # If it's a regular file, set the Content-Length\n            resp_stat = os.fstat(resp.raw.fileno())\n            if stat.S_ISREG(resp_stat.st_mode) and self._set_content_length:\n                resp.headers[\"Content-Length\"] = resp_stat.st_size\n\n        return resp\n\n    def close(self):\n        pass","metadata":{"execution":{"iopub.status.busy":"2021-05-28T19:39:42.487757Z","iopub.execute_input":"2021-05-28T19:39:42.488231Z","iopub.status.idle":"2021-05-28T19:39:42.510354Z","shell.execute_reply.started":"2021-05-28T19:39:42.488097Z","shell.execute_reply":"2021-05-28T19:39:42.509325Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"`tldextract` accurately separates the gTLD or ccTLD (generic or country code\ntop-level domain) from the registered domain and subdomains of a URL.\n    >>> import tldextract\n    >>> tldextract.extract('http://forums.news.cnn.com/')\n    ExtractResult(subdomain='forums.news', domain='cnn', suffix='com')\n    >>> tldextract.extract('http://forums.bbc.co.uk/') # United Kingdom\n    ExtractResult(subdomain='forums', domain='bbc', suffix='co.uk')\n    >>> tldextract.extract('http://www.worldbank.org.kg/') # Kyrgyzstan\n    ExtractResult(subdomain='www', domain='worldbank', suffix='org.kg')\n`ExtractResult` is a namedtuple, so it's simple to access the parts you want.\n    >>> ext = tldextract.extract('http://forums.bbc.co.uk')\n    >>> (ext.subdomain, ext.domain, ext.suffix)\n    ('forums', 'bbc', 'co.uk')\n    >>> # rejoin subdomain and domain\n    >>> '.'.join(ext[:2])\n    'forums.bbc'\n    >>> # a common alias\n    >>> ext.registered_domain\n    'bbc.co.uk'\nNote subdomain and suffix are _optional_. Not all URL-like inputs have a\nsubdomain or a valid suffix.\n    >>> tldextract.extract('google.com')\n    ExtractResult(subdomain='', domain='google', suffix='com')\n    >>> tldextract.extract('google.notavalidsuffix')\n    ExtractResult(subdomain='google', domain='notavalidsuffix', suffix='')\n    >>> tldextract.extract('http://127.0.0.1:8080/deployed/')\n    ExtractResult(subdomain='', domain='127.0.0.1', suffix='')\nIf you want to rejoin the whole namedtuple, regardless of whether a subdomain\nor suffix were found:\n    >>> ext = tldextract.extract('http://127.0.0.1:8080/deployed/')\n    >>> # this has unwanted dots\n    >>> '.'.join(ext)\n    '.127.0.0.1.'\n    >>> # join part only if truthy\n    >>> '.'.join(part for part in ext if part)\n    '127.0.0.1'\n\"\"\"\n\nimport collections\nimport logging\nimport os\nfrom functools import wraps\n\nimport idna\n\n\"\"\"Helpers \"\"\"\nimport errno\nimport hashlib\nimport json\nimport logging\nimport os\nimport os.path\nimport sys\nfrom hashlib import md5\n\nfrom filelock import FileLock\n\nLOG = logging.getLogger(__name__)\n\n_DID_LOG_UNABLE_TO_CACHE = False\n\n\ndef get_pkg_unique_identifier():\n    \"\"\"\n    Generate an identifier unique to the python version, tldextract version, and python instance\n\n    This will prevent interference between virtualenvs and issues that might arise when installing\n    a new version of tldextract\n    \"\"\"\n    try:\n        # pylint: disable=import-outside-toplevel\n        from tldextract._version import version\n    except ImportError:\n        version = \"dev\"\n\n    tldextract_version = \"tldextract-\" + version\n    python_env_name = os.path.basename(sys.prefix)\n    # just to handle the edge case of two identically named python environments\n    python_binary_path_short_hash = hashlib.md5(sys.prefix.encode(\"utf-8\")).hexdigest()[:6]\n    python_version = \".\".join([str(v) for v in sys.version_info[:-1]])\n    identifier_parts = [\n        python_version,\n        python_env_name,\n        python_binary_path_short_hash,\n        tldextract_version\n    ]\n    pkg_identifier = \"__\".join(identifier_parts)\n\n    return pkg_identifier\n\n\ndef get_cache_dir():\n    \"\"\"\n    Get a cache dir that we have permission to write to\n\n    Try to follow the XDG standard, but if that doesn't work fallback to the package directory\n    http://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html\n    \"\"\"\n    cache_dir = os.environ.get(\"TLDEXTRACT_CACHE\", None)\n    if cache_dir is not None:\n        return cache_dir\n\n    xdg_cache_home = os.getenv(\"XDG_CACHE_HOME\", None)\n    if xdg_cache_home is None:\n        user_home = os.getenv(\"HOME\", None)\n        if user_home:\n            xdg_cache_home = os.path.join(user_home, \".cache\")\n\n    if xdg_cache_home is not None:\n        return os.path.join(xdg_cache_home, \"python-tldextract\", get_pkg_unique_identifier())\n\n    # fallback to trying to use package directory itself\n    return os.path.join(os.path.dirname(__file__), \".suffix_cache/\")\n\n\nclass DiskCache:\n    \"\"\"Disk _cache that only works for jsonable values\"\"\"\n\n    def __init__(self, cache_dir, lock_timeout=20):\n        self.enabled = bool(cache_dir)\n        self.cache_dir = os.path.expanduser(str(cache_dir) or \"\")\n        self.lock_timeout = lock_timeout\n        # using a unique extension provides some safety that an incorrectly set cache_dir\n        # combined with a call to `.clear()` wont wipe someones hard drive\n        self.file_ext = \".tldextract.json\"\n\n    def get(self, namespace, key):\n        \"\"\"Retrieve a value from the disk cache\"\"\"\n        if not self.enabled:\n            raise KeyError(\"Cache is disabled\")\n        cache_filepath = self._key_to_cachefile_path(namespace, key)\n\n        if not os.path.isfile(cache_filepath):\n            raise KeyError(\"namespace: \" + namespace + \" key: \" + repr(key))\n        try:\n            with open(cache_filepath) as cache_file:\n                return json.load(cache_file)\n        except (OSError, ValueError) as exc:\n            LOG.error(\"error reading TLD cache file %s: %s\", cache_filepath, exc)\n            raise KeyError(  # pylint: disable=raise-missing-from\n                \"namespace: \" + namespace + \" key: \" + repr(key)\n            )\n\n    def set(self, namespace, key, value):\n        \"\"\"Set a value in the disk cache\"\"\"\n        if not self.enabled:\n            return False\n        cache_filepath = self._key_to_cachefile_path(namespace, key)\n\n        try:\n            _make_dir(cache_filepath)\n            with open(cache_filepath, \"w\") as cache_file:\n                json.dump(value, cache_file)\n        except OSError as ioe:\n            global _DID_LOG_UNABLE_TO_CACHE  # pylint: disable=global-statement\n            if not _DID_LOG_UNABLE_TO_CACHE:\n                LOG.warning(\n                    (\n                        \"unable to cache %s.%s in %s. This could refresh the \"\n                        \"Public Suffix List over HTTP every app startup. \"\n                        \"Construct your `TLDExtract` with a writable `cache_dir` or \"\n                        \"set `cache_dir=False` to silence this warning. %s\"\n                    ),\n                    namespace,\n                    key,\n                    cache_filepath,\n                    ioe,\n                )\n                _DID_LOG_UNABLE_TO_CACHE = True\n\n        return None\n\n    def clear(self):\n        \"\"\"Clear the disk cache\"\"\"\n        for root, _, files in os.walk(self.cache_dir):\n            for filename in files:\n                if filename.endswith(self.file_ext) or filename.endswith(\n                    self.file_ext + \".lock\"\n                ):\n                    try:\n                        os.unlink(os.path.join(root, filename))\n                    except FileNotFoundError:\n                        pass\n                    except OSError as exc:\n                        # errno.ENOENT == \"No such file or directory\"\n                        # https://docs.python.org/2/library/errno.html#errno.ENOENT\n                        if exc.errno != errno.ENOENT:\n                            raise\n\n    def _key_to_cachefile_path(self, namespace, key):\n        namespace_path = os.path.join(self.cache_dir, namespace)\n        hashed_key = _make_cache_key(key)\n\n        cache_path = os.path.join(namespace_path, hashed_key + self.file_ext)\n\n        return cache_path\n\n    def run_and_cache(self, func, namespace, kwargs, hashed_argnames):\n        \"\"\"Get a url but cache the response\"\"\"\n        if not self.enabled:\n            return func(**kwargs)\n\n        key_args = {k: v for k, v in kwargs.items() if k in hashed_argnames}\n        cache_filepath = self._key_to_cachefile_path(namespace, key_args)\n        lock_path = cache_filepath + \".lock\"\n        try:\n            _make_dir(cache_filepath)\n        except OSError as ioe:\n            global _DID_LOG_UNABLE_TO_CACHE  # pylint: disable=global-statement\n            if not _DID_LOG_UNABLE_TO_CACHE:\n                LOG.warning(\n                    (\n                        \"unable to cache %s.%s in %s. This could refresh the \"\n                        \"Public Suffix List over HTTP every app startup. \"\n                        \"Construct your `TLDExtract` with a writable `cache_dir` or \"\n                        \"set `cache_dir=False` to silence this warning. %s\"\n                    ),\n                    namespace,\n                    key_args,\n                    cache_filepath,\n                    ioe,\n                )\n                _DID_LOG_UNABLE_TO_CACHE = True\n\n            return func(**kwargs)\n\n        with FileLock(lock_path, timeout=self.lock_timeout):\n            try:\n                result = self.get(namespace=namespace, key=key_args)\n            except KeyError:\n                result = func(**kwargs)\n                self.set(namespace=\"urls\", key=key_args, value=result)\n\n            return result\n\n    def cached_fetch_url(self, session, url, timeout):\n        \"\"\"Get a url but cache the response\"\"\"\n        return self.run_and_cache(\n            func=_fetch_url,\n            namespace=\"urls\",\n            kwargs={\"session\": session, \"url\": url, \"timeout\": timeout},\n            hashed_argnames=[\"url\"],\n        )\n\n\ndef _fetch_url(session, url, timeout):\n\n    response = session.get(url, timeout=timeout)\n    response.raise_for_status()\n    text = response.text\n\n    if not isinstance(text, str):\n        text = str(text, \"utf-8\")\n\n    return text\n\n\ndef _make_cache_key(inputs):\n    key = repr(inputs)\n    try:\n        key = md5(key).hexdigest()\n    except TypeError:\n        key = md5(key.encode(\"utf8\")).hexdigest()\n    return key\n\n\ndef _make_dir(filename):\n    \"\"\"Make a directory if it doesn't already exist\"\"\"\n    if not os.path.exists(os.path.dirname(filename)):\n        try:\n            os.makedirs(os.path.dirname(filename))\n        except OSError as exc:  # Guard against race condition\n            if exc.errno != errno.EEXIST:\n                raise\n'tldextract helpers for testing and fetching remote resources.'\n\nimport re\nimport socket\n\nfrom urllib.parse import scheme_chars\n\n\nIP_RE = re.compile(\n    r'^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])$')  # pylint: disable=line-too-long\n\nSCHEME_RE = re.compile(r'^([' + scheme_chars + ']+:)?//')\n\n\ndef looks_like_ip(maybe_ip):\n    \"\"\"Does the given str look like an IP address?\"\"\"\n    if not maybe_ip[0].isdigit():\n        return False\n\n    try:\n        socket.inet_aton(maybe_ip)\n        return True\n    except (AttributeError, UnicodeError):\n        if IP_RE.match(maybe_ip):\n            return True\n    except socket.error:\n        return False\n\"tldextract helpers for testing and fetching remote resources.\"\n\nimport logging\nimport pkgutil\nimport re\n\nimport requests\n# from requests_file import FileAdapter\n\nLOG = logging.getLogger(\"tldextract\")\n\nPUBLIC_SUFFIX_RE = re.compile(r\"^(?P<suffix>[.*!]*\\w[\\S]*)\", re.UNICODE | re.MULTILINE)\nPUBLIC_PRIVATE_SUFFIX_SEPARATOR = \"// ===BEGIN PRIVATE DOMAINS===\"\n\n\nclass SuffixListNotFound(LookupError):\n    \"\"\"A recoverable error while looking up a suffix list. Recoverable because\n    you can specify backups, or use this library's bundled snapshot.\"\"\"\n\n\ndef find_first_response(cache, urls, cache_fetch_timeout=None):\n    \"\"\"Decode the first successfully fetched URL, from UTF-8 encoding to\n    Python unicode.\n    \"\"\"\n    with requests.Session() as session:\n        session.mount(\"file://\", FileAdapter())\n\n        for url in urls:\n            try:\n                return cache.cached_fetch_url(\n                    session=session, url=url, timeout=cache_fetch_timeout\n                )\n            except requests.exceptions.RequestException:\n                LOG.exception(\"Exception reading Public Suffix List url %s\", url)\n    raise SuffixListNotFound(\n        \"No Public Suffix List found. Consider using a mirror or constructing \"\n        \"your TLDExtract with `suffix_list_urls=None`.\"\n    )\n\n\ndef extract_tlds_from_suffix_list(suffix_list_text):\n    \"\"\"Parse the raw suffix list text for its different designations of\n    suffixes.\"\"\"\n    public_text, _, private_text = suffix_list_text.partition(\n        PUBLIC_PRIVATE_SUFFIX_SEPARATOR\n    )\n\n    public_tlds = [m.group(\"suffix\") for m in PUBLIC_SUFFIX_RE.finditer(public_text)]\n    private_tlds = [m.group(\"suffix\") for m in PUBLIC_SUFFIX_RE.finditer(private_text)]\n    return public_tlds, private_tlds\n\n\ndef get_suffix_lists(cache, urls, cache_fetch_timeout, fallback_to_snapshot):\n    \"\"\"Fetch, parse, and cache the suffix lists\"\"\"\n    return cache.run_and_cache(\n        func=_get_suffix_lists,\n        namespace=\"publicsuffix.org-tlds\",\n        kwargs={\n            \"cache\": cache,\n            \"urls\": urls,\n            \"cache_fetch_timeout\": cache_fetch_timeout,\n            \"fallback_to_snapshot\": fallback_to_snapshot,\n        },\n        hashed_argnames=[\"urls\", \"fallback_to_snapshot\"],\n    )\n\n\ndef _get_suffix_lists(cache, urls, cache_fetch_timeout, fallback_to_snapshot):\n    \"\"\"Fetch, parse, and cache the suffix lists\"\"\"\n\n    try:\n        text = find_first_response(cache, urls, cache_fetch_timeout=cache_fetch_timeout)\n    except SuffixListNotFound as exc:\n        if fallback_to_snapshot:\n            text = pkgutil.get_data(\"tldextract\", \".tld_set_snapshot\")\n            if not isinstance(text, str):\n                text = str(text, \"utf-8\")\n        else:\n            raise exc\n\n    public_tlds, private_tlds = extract_tlds_from_suffix_list(text)\n\n    return public_tlds, private_tlds\n\nLOG = logging.getLogger(\"tldextract\")\n\n\nCACHE_TIMEOUT = os.environ.get(\"TLDEXTRACT_CACHE_TIMEOUT\")\n\nPUBLIC_SUFFIX_LIST_URLS = (\n    \"https://publicsuffix.org/list/public_suffix_list.dat\",\n    \"https://raw.githubusercontent.com/publicsuffix/list/master/public_suffix_list.dat\",\n)\n\n\nclass ExtractResult(collections.namedtuple(\"ExtractResult\", \"subdomain domain suffix\")):\n    \"\"\"namedtuple of a URL's subdomain, domain, and suffix.\"\"\"\n\n    # Necessary for __dict__ member to get populated in Python 3+\n    __slots__ = ()\n\n    @property\n    def registered_domain(self):\n        \"\"\"\n        Joins the domain and suffix fields with a dot, if they're both set.\n        >>> extract('http://forums.bbc.co.uk').registered_domain\n        'bbc.co.uk'\n        >>> extract('http://localhost:8080').registered_domain\n        ''\n        \"\"\"\n        if self.domain and self.suffix:\n            return self.domain + \".\" + self.suffix\n        return \"\"\n\n    @property\n    def fqdn(self):\n        \"\"\"\n        Returns a Fully Qualified Domain Name, if there is a proper domain/suffix.\n        >>> extract('http://forums.bbc.co.uk/path/to/file').fqdn\n        'forums.bbc.co.uk'\n        >>> extract('http://localhost:8080').fqdn\n        ''\n        \"\"\"\n        if self.domain and self.suffix:\n            # self is the namedtuple (subdomain domain suffix)\n            return \".\".join(i for i in self if i)\n        return \"\"\n\n    @property\n    def ipv4(self):\n        \"\"\"\n        Returns the ipv4 if that is what the presented domain/url is\n        >>> extract('http://127.0.0.1/path/to/file').ipv4\n        '127.0.0.1'\n        >>> extract('http://127.0.0.1.1/path/to/file').ipv4\n        ''\n        >>> extract('http://256.1.1.1').ipv4\n        ''\n        \"\"\"\n        if not (self.suffix or self.subdomain) and IP_RE.match(self.domain):\n            return self.domain\n        return \"\"\n\n\nclass TLDExtract:\n    \"\"\"A callable for extracting, subdomain, domain, and suffix components from\n    a URL.\"\"\"\n\n    # TODO: Agreed with Pylint: too-many-arguments\n    def __init__(  # pylint: disable=too-many-arguments\n        self,\n        cache_dir=get_cache_dir(),\n        suffix_list_urls=PUBLIC_SUFFIX_LIST_URLS,\n        fallback_to_snapshot=True,\n        include_psl_private_domains=False,\n        extra_suffixes=(),\n        cache_fetch_timeout=CACHE_TIMEOUT,\n    ):\n        \"\"\"\n        Constructs a callable for extracting subdomain, domain, and suffix\n        components from a URL.\n        Upon calling it, it first checks for a JSON in `cache_dir`.\n        By default, the `cache_dir` will live in the tldextract directory.\n        You can disable the caching functionality of this module  by setting `cache_dir` to False.\n        If the cached version does not exist (such as on the first run), HTTP request the URLs in\n        `suffix_list_urls` in order, until one returns public suffix list data. To disable HTTP\n        requests, set this to something falsy.\n        The default list of URLs point to the latest version of the Mozilla Public Suffix List and\n        its mirror, but any similar document could be specified. Local files can be specified by\n        using the `file://` protocol. (See `urllib2` documentation.)\n        If there is no cached version loaded and no data is found from the `suffix_list_urls`,\n        the module will fall back to the included TLD set snapshot. If you do not want\n        this behavior, you may set `fallback_to_snapshot` to False, and an exception will be\n        raised instead.\n        The Public Suffix List includes a list of \"private domains\" as TLDs,\n        such as blogspot.com. These do not fit `tldextract`'s definition of a\n        suffix, so these domains are excluded by default. If you'd like them\n        included instead, set `include_psl_private_domains` to True.\n        You can pass additional suffixes in `extra_suffixes` argument without changing list URL\n        cache_fetch_timeout is passed unmodified to the underlying request object\n        per the requests documentation here:\n        http://docs.python-requests.org/en/master/user/advanced/#timeouts\n        cache_fetch_timeout can also be set to a single value with the\n        environment variable TLDEXTRACT_CACHE_TIMEOUT, like so:\n        TLDEXTRACT_CACHE_TIMEOUT=\"1.2\"\n        When set this way, the same timeout value will be used for both connect\n        and read timeouts\n        \"\"\"\n        suffix_list_urls = suffix_list_urls or ()\n        self.suffix_list_urls = tuple(\n            url.strip() for url in suffix_list_urls if url.strip()\n        )\n\n        self.fallback_to_snapshot = fallback_to_snapshot\n        if not (self.suffix_list_urls or cache_dir or self.fallback_to_snapshot):\n            raise ValueError(\n                \"The arguments you have provided disable all ways for tldextract \"\n                \"to obtain data. Please provide a suffix list data, a cache_dir, \"\n                \"or set `fallback_to_snapshot` to `True`.\"\n            )\n\n        self.include_psl_private_domains = include_psl_private_domains\n        self.extra_suffixes = extra_suffixes\n        self._extractor = None\n\n        self.cache_fetch_timeout = cache_fetch_timeout\n        self._cache = DiskCache(cache_dir)\n        if isinstance(self.cache_fetch_timeout, str):\n            self.cache_fetch_timeout = float(self.cache_fetch_timeout)\n\n    def __call__(self, url, include_psl_private_domains=None):\n        \"\"\"\n        Takes a string URL and splits it into its subdomain, domain, and\n        suffix (effective TLD, gTLD, ccTLD, etc.) component.\n        >>> extract = TLDExtract()\n        >>> extract('http://forums.news.cnn.com/')\n        ExtractResult(subdomain='forums.news', domain='cnn', suffix='com')\n        >>> extract('http://forums.bbc.co.uk/')\n        ExtractResult(subdomain='forums', domain='bbc', suffix='co.uk')\n        \"\"\"\n\n        netloc = (\n            SCHEME_RE.sub(\"\", url)\n            .partition(\"/\")[0]\n            .partition(\"?\")[0]\n            .partition(\"#\")[0]\n            .split(\"@\")[-1]\n            .partition(\":\")[0]\n            .strip()\n            .rstrip(\".\")\n        )\n\n        labels = netloc.split(\".\")\n\n        translations = [_decode_punycode(label) for label in labels]\n        suffix_index = self._get_tld_extractor().suffix_index(\n            translations, include_psl_private_domains=include_psl_private_domains\n        )\n\n        suffix = \".\".join(labels[suffix_index:])\n        if not suffix and netloc and looks_like_ip(netloc):\n            return ExtractResult(\"\", netloc, \"\")\n\n        subdomain = \".\".join(labels[: suffix_index - 1]) if suffix_index else \"\"\n        domain = labels[suffix_index - 1] if suffix_index else \"\"\n        return ExtractResult(subdomain, domain, suffix)\n\n    def update(self, fetch_now=False):\n        \"\"\"Force fetch the latest suffix list definitions.\"\"\"\n        self._extractor = None\n        self._cache.clear()\n        if fetch_now:\n            self._get_tld_extractor()\n\n    @property\n    def tlds(self):\n        \"\"\"\n        Returns the list of tld's used by default\n        This will vary based on `include_psl_private_domains` and `extra_suffixes`\n        \"\"\"\n        return list(self._get_tld_extractor().tlds())\n\n    def _get_tld_extractor(self):\n        \"\"\"Get or compute this object's TLDExtractor. Looks up the TLDExtractor\n        in roughly the following order, based on the settings passed to\n        __init__:\n        1. Memoized on `self`\n        2. Local system _cache file\n        3. Remote PSL, over HTTP\n        4. Bundled PSL snapshot file\"\"\"\n\n        if self._extractor:\n            return self._extractor\n\n        public_tlds, private_tlds = get_suffix_lists(\n            cache=self._cache,\n            urls=self.suffix_list_urls,\n            cache_fetch_timeout=self.cache_fetch_timeout,\n            fallback_to_snapshot=self.fallback_to_snapshot,\n        )\n\n        if not any([public_tlds, private_tlds, self.extra_suffixes]):\n            raise ValueError(\"No tlds set. Cannot proceed without tlds.\")\n\n        self._extractor = _PublicSuffixListTLDExtractor(\n            public_tlds=public_tlds,\n            private_tlds=private_tlds,\n            extra_tlds=list(self.extra_suffixes),\n            include_psl_private_domains=self.include_psl_private_domains,\n        )\n        return self._extractor\n\n\nTLD_EXTRACTOR = TLDExtract()\n\n\n@wraps(TLD_EXTRACTOR.__call__)\ndef extract(\n    url, include_psl_private_domains=False\n):  # pylint: disable=missing-function-docstring\n    return TLD_EXTRACTOR(url, include_psl_private_domains=include_psl_private_domains)\n\n\n@wraps(TLD_EXTRACTOR.update)\ndef update(*args, **kwargs):  # pylint: disable=missing-function-docstring\n    return TLD_EXTRACTOR.update(*args, **kwargs)\n\n\nclass _PublicSuffixListTLDExtractor:\n    \"\"\"Wrapper around this project's main algo for PSL\n    lookups.\n    \"\"\"\n\n    def __init__(\n        self, public_tlds, private_tlds, extra_tlds, include_psl_private_domains=False\n    ):\n        # set the default value\n        self.include_psl_private_domains = include_psl_private_domains\n        self.public_tlds = public_tlds\n        self.private_tlds = private_tlds\n        self.tlds_incl_private = frozenset(public_tlds + private_tlds + extra_tlds)\n        self.tlds_excl_private = frozenset(public_tlds + extra_tlds)\n\n    def tlds(self, include_psl_private_domains=None):\n        \"\"\"Get the currently filtered list of suffixes.\"\"\"\n        if include_psl_private_domains is None:\n            include_psl_private_domains = self.include_psl_private_domains\n\n        return (\n            self.tlds_incl_private\n            if include_psl_private_domains\n            else self.tlds_excl_private\n        )\n\n    def suffix_index(self, lower_spl, include_psl_private_domains=None):\n        \"\"\"Returns the index of the first suffix label.\n        Returns len(spl) if no suffix is found\n        \"\"\"\n        tlds = self.tlds(include_psl_private_domains)\n        length = len(lower_spl)\n        for i in range(length):\n            maybe_tld = \".\".join(lower_spl[i:])\n            exception_tld = \"!\" + maybe_tld\n            if exception_tld in tlds:\n                return i + 1\n\n            if maybe_tld in tlds:\n                return i\n\n            wildcard_tld = \"*.\" + \".\".join(lower_spl[i + 1 :])\n            if wildcard_tld in tlds:\n                return i\n\n        return length\n\n\ndef _decode_punycode(label):\n    lowered = label.lower()\n    looks_like_puny = lowered.startswith(\"xn--\")\n    if looks_like_puny:\n        try:\n            return idna.decode(label.encode(\"ascii\")).lower()\n        except (UnicodeError, IndexError):\n            pass\n    return lowered","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-05-28T19:39:42.521862Z","iopub.execute_input":"2021-05-28T19:39:42.522221Z","iopub.status.idle":"2021-05-28T19:39:42.594846Z","shell.execute_reply.started":"2021-05-28T19:39:42.522194Z","shell.execute_reply":"2021-05-28T19:39:42.593894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport random\nimport gc\nimport tqdm\n\nimport pandas as pd\nimport numpy as np\nimport spacy\nimport sys\nsys.path = [\n    '../input/readability-package/',\n] + sys.path\nimport readability\nimport nltk\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nfrom nltk import pos_tag, pos_tag_sents\nfrom urllib.parse import urlparse\nimport re\n\nfrom sklearn import metrics, preprocessing, model_selection\nimport lightgbm as lgb\nimport copy\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport random\nfrom sklearn.preprocessing import LabelEncoder\n\n\nfrom sklearn import model_selection\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport copy\nfrom sklearn import metrics\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import mean_squared_error\n\nimport lightgbm as lgb\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam, lr_scheduler\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import (AutoModel, AutoTokenizer, \n                          AutoModelForSequenceClassification,get_constant_schedule_with_warmup)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T19:39:42.596308Z","iopub.execute_input":"2021-05-28T19:39:42.596594Z","iopub.status.idle":"2021-05-28T19:39:46.78298Z","shell.execute_reply.started":"2021-05-28T19:39:42.596567Z","shell.execute_reply":"2021-05-28T19:39:46.782079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set Configs/Constants\n\nclass config:\n    \n    SEED = 42\n    MAX_LEN = 256\n    TRAIN_BATCH_SIZE = 128\n    VAL_BATCH_SIZE = 64\n    ROBERTA_MODEL_PATH = '../input/roberta-base'\n    EPOCHS = 3\n    LR = 1e-5\n    TEXT_COLUMN = 'excerpt'","metadata":{"execution":{"iopub.status.busy":"2021-05-28T19:39:46.784813Z","iopub.execute_input":"2021-05-28T19:39:46.785122Z","iopub.status.idle":"2021-05-28T19:39:46.792934Z","shell.execute_reply.started":"2021-05-28T19:39:46.785095Z","shell.execute_reply":"2021-05-28T19:39:46.792077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed = 0):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random_state = np.random.RandomState(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    return random_state\n\nrandom_state = set_seed(config.SEED)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T19:39:46.794494Z","iopub.execute_input":"2021-05-28T19:39:46.795022Z","iopub.status.idle":"2021-05-28T19:39:46.808928Z","shell.execute_reply.started":"2021-05-28T19:39:46.794984Z","shell.execute_reply":"2021-05-28T19:39:46.808075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reading Data\npd.options.display.max_rows = 4000\ntrain = pd.read_csv('../input/commonlitreadabilityprize/train.csv', low_memory=False)\ntest = pd.read_csv('../input/commonlitreadabilityprize/test.csv', low_memory=False)\nsample = pd.read_csv('../input/commonlitreadabilityprize/sample_submission.csv', low_memory=False)\n\ntarget = train['target'].to_numpy()","metadata":{"execution":{"iopub.status.busy":"2021-05-28T19:39:46.810247Z","iopub.execute_input":"2021-05-28T19:39:46.81071Z","iopub.status.idle":"2021-05-28T19:39:46.85605Z","shell.execute_reply.started":"2021-05-28T19:39:46.810673Z","shell.execute_reply":"2021-05-28T19:39:46.855239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Taken this from https://www.kaggle.com/ravishah1/readability-feature-engineering-non-nn-baseline\n# from tldextract import extract\n\ndef readability_measurements(passage: str):\n    \"\"\"\n    This function uses the readability library for feature engineering.\n    It includes textual statistics, readability scales and metric, and some pos stats\n    \"\"\"\n    results = readability.getmeasures(passage, lang='en')\n    \n    chars_per_word = results['sentence info']['characters_per_word']\n    syll_per_word = results['sentence info']['syll_per_word']\n    words_per_sent = results['sentence info']['words_per_sentence']\n    \n    kincaid = results['readability grades']['Kincaid']\n    ari = results['readability grades']['ARI']\n    coleman_liau = results['readability grades']['Coleman-Liau']\n    flesch = results['readability grades']['FleschReadingEase']\n    gunning_fog = results['readability grades']['GunningFogIndex']\n    lix = results['readability grades']['LIX']\n    smog = results['readability grades']['SMOGIndex']\n    rix = results['readability grades']['RIX']\n    dale_chall = results['readability grades']['DaleChallIndex']\n    \n    tobeverb = results['word usage']['tobeverb']\n    auxverb = results['word usage']['auxverb']\n    conjunction = results['word usage']['conjunction']\n    pronoun = results['word usage']['pronoun']\n    preposition = results['word usage']['preposition']\n    nominalization = results['word usage']['nominalization']\n    \n    pronoun_b = results['sentence beginnings']['pronoun']\n    interrogative = results['sentence beginnings']['interrogative']\n    article = results['sentence beginnings']['article']\n    subordination = results['sentence beginnings']['subordination']\n    conjunction_b = results['sentence beginnings']['conjunction']\n    preposition_b = results['sentence beginnings']['preposition']\n\n    \n    return [chars_per_word, syll_per_word, words_per_sent,\n            kincaid, ari, coleman_liau, flesch, gunning_fog, lix, smog, rix, dale_chall,\n            tobeverb, auxverb, conjunction, pronoun, preposition, nominalization,\n            pronoun_b, interrogative, article, subordination, conjunction_b, preposition_b]\n\n# Taken this from https://www.kaggle.com/ravishah1/readability-feature-engineering-non-nn-baseline\ndef spacy_features(df: pd.DataFrame):\n    \"\"\"\n    This function generates features using spacy en_core_wb_lg\n    I learned about this from these resources:\n    https://www.kaggle.com/konradb/linear-baseline-with-cv\n    https://www.kaggle.com/anaverageengineer/comlrp-baseline-for-complete-beginners\n    \"\"\"\n    \n    nlp = spacy.load('en_core_web_lg')\n    with nlp.disable_pipes():\n        vectors = np.array([nlp(text).vector for text in df.excerpt])\n        \n    return vectors\n\ndef get_spacy_col_names():\n    names = list()\n    for i in range(300):\n        names.append(f\"spacy_{i}\")\n        \n    return names\n\n# Taken this from https://www.kaggle.com/ravishah1/readability-feature-engineering-non-nn-baseline\ndef pos_tag_features(passage: str):\n    \"\"\"\n    This function counts the number of times different parts of speech occur in an excerpt\n    \"\"\"\n    pos_tags = [\"CC\", \"CD\", \"DT\", \"EX\", \"FW\", \"IN\", \"JJ\", \"JJR\", \"JJS\", \"LS\", \"MD\", \n                \"NN\", \"NNS\", \"NNP\", \"NNPS\", \"PDT\", \"POS\", \"PRP\", \"RB\", \"RBR\", \"RBS\", \"RP\", \"TO\", \"UH\",\n                \"VB\", \"VBD\", \"VBG\", \"VBZ\", \"WDT\", \"WP\", \"WRB\"]\n    \n    tags = pos_tag(word_tokenize(passage))\n    tag_list= list()\n    \n    for tag in pos_tags:\n        tag_list.append(len([i[0] for i in tags if i[1] == tag]))\n    \n    return tag_list\n\n# Taken this from https://www.kaggle.com/ravishah1/readability-feature-engineering-non-nn-baseline\ndef generate_other_features(passage: str):\n    \"\"\"\n    This function is where I test miscellaneous features\n    This is experimental\n    \"\"\"\n    # punctuation count\n    periods = passage.count(\".\")\n    commas = passage.count(\",\")\n    semis = passage.count(\";\")\n    exclaims = passage.count(\"!\")\n    questions = passage.count(\"?\")\n    \n    # Some other stats\n    num_char = len(passage)\n    num_words = len(passage.split(\" \"))\n    unique_words = len(set(passage.split(\" \") ))\n    word_diversity = unique_words/num_words\n    \n    word_len = [len(w) for w in passage.split(\" \")]\n    longest_word = np.max(word_len)\n    avg_len_word = np.mean(word_len)\n    \n    return [periods, commas, semis, exclaims, questions,\n            num_char, num_words, unique_words, word_diversity,\n            longest_word, avg_len_word]\n\ndef extract_features(df):\n\n    scores_df = pd.DataFrame(df[\"excerpt\"].apply(lambda p : readability_measurements(p)).tolist(), \n                                 columns=[\"chars_per_word\", \"syll_per_word\", \"words_per_sent\",\n                                          \"kincaid\", \"ari\", \"coleman_liau\", \"flesch\", \"gunning_fog\", \"lix\", \"smog\", \"rix\", \"dale_chall\",\n                                          \"tobeverb\", \"auxverb\", \"conjunction\", \"pronoun\", \"preposition\", \"nominalization\",\n                                          \"pronoun_b\", \"interrogative\", \"article\", \"subordination\", \"conjunction_b\", \"preposition_b\"])\n    df = pd.merge(df, scores_df, left_index=True, right_index=True)\n    \n    spacy_df = pd.DataFrame(spacy_features(df), columns=get_spacy_col_names())\n    df = pd.merge(df, spacy_df, left_index=True, right_index=True)\n    \n    pos_df = pd.DataFrame(df[\"excerpt\"].apply(lambda p : pos_tag_features(p)).tolist(),\n                            columns=[\"CC\", \"CD\", \"DT\", \"EX\", \"FW\", \"IN\", \"JJ\", \"JJR\", \"JJS\", \"LS\", \"MD\", \n                                    \"NN\", \"NNS\", \"NNP\", \"NNPS\", \"PDT\", \"POS\", \"PRP\", \"RB\", \"RBR\", \"RBS\", \"RP\", \"TO\", \"UH\",\n                                    \"VB\", \"VBD\", \"VBG\", \"VBZ\", \"WDT\", \"WP\", \"WRB\"])\n    df = pd.merge(df, pos_df, left_index=True, right_index=True)\n    \n    other_df = pd.DataFrame(df[\"excerpt\"].apply(lambda p : generate_other_features(p)).tolist(),\n                            columns=[\"periods\", \"commas\", \"semis\", \"exclaims\", \"questions\",\n                                        \"num_char\", \"num_words\", \"unique_words\", \"word_diversity\",\n                                        \"longest_word\", \"avg_len_word\"])\n    df = pd.merge(df, other_df, left_index=True, right_index=True)\n\n    return df\n\ndef extract_url_license_feat(df):\n    temp = pd.DataFrame()\n    temp['article_year'] = df['url_legal'].apply(lambda x : x if x is np.nan else re.search('(2\\d{3})|$', urlparse(x).path).group())\n    temp['subdomain'] = df['url_legal'].apply(lambda x : x if x is np.nan else extract(x)[0])\n    temp['domain'] = df['url_legal'].apply(lambda x : x if x is np.nan else extract(x)[1])\n    temp['suffix'] = df['url_legal'].apply(lambda x : x if x is np.nan else extract(x)[2])\n    temp['is_pdf'] = df['url_legal'].apply(lambda x : x if x is np.nan else ('Y' if '.pdf' in str(x) else 'N'))\n    \n    temp['is_cc'] = df['license'].apply(lambda x : x if x is np.nan else ('Y' if 'CC' in str(x) else 'N'))\n    temp['is_by'] = df['license'].apply(lambda x : x if x is np.nan else ('Y' if 'BY' in str(x) else 'N'))\n    temp['is_sa'] = df['license'].apply(lambda x : x if x is np.nan else ('Y' if 'SA' in str(x) else 'N'))\n    temp['is_nc'] = df['license'].apply(lambda x : x if x is np.nan else ('Y' if 'NC' in str(x) else 'N'))\n    temp['is_nd'] = df['license'].apply(lambda x : x if x is np.nan else ('Y' if 'ND' in str(x) else 'N'))\n    temp['is_gnu'] = df['license'].apply(lambda x : x if x is np.nan else ('Y' if 'GNU' in str(x) else 'N'))\n    temp['license_version'] = df['license'].apply(lambda x : x if x is np.nan else(float(0) if re.search('([0-9][.][0-9])|$', urlparse(x).path).group() == '' else float(re.search('([0-9][.][0-9])|$', urlparse(x).path).group())))\n    df = pd.concat([df, temp], axis = 1)\n    return df\n\ndef handle_cate_NA(df, columns_to_ignore=[]):\n    temp = copy.deepcopy(df)\n    cate_cols = list(set(temp.select_dtypes('object').columns.tolist()) - set(columns_to_ignore))\n    for col in cate_cols:\n        if temp[col].isna().sum() > 0:\n            column_name = 'NA_POS_'+col\n            col_values = ['Y' if pd.isna(value[1]) else 'N' for value in df[col].items()]\n            temp[col].fillna(value='ABS', inplace=True)\n            temp[column_name] = col_values\n    return temp\n\ndef handle_cont_NA(df, method='mean'):\n    action = ''.join(c.lower() for c in method if not c.isspace())\n    temp = copy.deepcopy(df)\n    num_cols = temp.select_dtypes(include='number')\n    for col in num_cols:\n        if temp[col].isna().sum() > 0:\n            column_name = 'NA_POS_'+col\n            col_values = ['Y' if pd.isna(value[1]) else 'N' for value in df[col].items()]\n            #value_if_true if condition else value_if_false\n            fill_value = np.mean(temp[col]) if 'mean' == action else np.median(temp[col])\n            temp[col].fillna(value = fill_value, inplace=True)\n            temp[column_name] = col_values\n    return temp\n\ndef train_pca(df, list_of_columns, column_prefix):\n    temp = copy.deepcopy(df)\n    x = temp.loc[:, list_of_columns].values\n    ss = StandardScaler().fit(x)\n    x = ss.transform(x)\n    pca = PCA(n_components=2)\n    pca.fit(x)\n    principalComponents = pca.transform(x)\n#     print(column_prefix, pca.explained_variance_ratio_)\n    principalDf = pd.DataFrame(data = principalComponents, columns = [column_prefix+'_1', column_prefix+'_2'])\n#     temp.drop(columns=list_of_columns, axis=1, inplace=True)\n    temp = pd.concat([temp, principalDf], axis = 1)\n    result_dict = { 'pca': pca, 'ss': ss, 'list_of_columns': list_of_columns, 'column_prefix': column_prefix } \n    return result_dict, temp\n\ndef apply_pca(trained_pca, df):\n    temp = copy.deepcopy(df)\n    x = temp.loc[:, trained_pca.get('list_of_columns')].values\n    x = trained_pca.get('ss').transform(x)\n    principalComponents = trained_pca.get('pca').transform(x)\n    principalDf = pd.DataFrame(data = principalComponents, columns = [trained_pca.get('column_prefix')+'_1', trained_pca.get('column_prefix')+'_2'])\n#     temp.drop(columns=trained_pca.get('list_of_columns'), axis=1, inplace=True)\n    temp = pd.concat([temp, principalDf], axis = 1)\n    return temp","metadata":{"execution":{"iopub.status.busy":"2021-05-28T19:39:46.857567Z","iopub.execute_input":"2021-05-28T19:39:46.857946Z","iopub.status.idle":"2021-05-28T19:39:46.903684Z","shell.execute_reply.started":"2021-05-28T19:39:46.85791Z","shell.execute_reply":"2021-05-28T19:39:46.90279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = extract_features(train)\n# train = extract_url_license_feat(train)\ntrain = handle_cate_NA(train)\ntrain = handle_cont_NA(train)\ntrain.shape","metadata":{"execution":{"iopub.status.busy":"2021-05-28T19:39:46.906596Z","iopub.execute_input":"2021-05-28T19:39:46.906883Z","iopub.status.idle":"2021-05-28T19:41:48.51612Z","shell.execute_reply.started":"2021-05-28T19:39:46.906858Z","shell.execute_reply":"2021-05-28T19:41:48.515272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = extract_features(test)\n# test = extract_url_license_feat(test)\ntest = handle_cate_NA(test)\ntest = handle_cont_NA(test)\ntest.shape","metadata":{"execution":{"iopub.status.busy":"2021-05-28T19:41:48.519714Z","iopub.execute_input":"2021-05-28T19:41:48.52167Z","iopub.status.idle":"2021-05-28T19:41:54.51897Z","shell.execute_reply.started":"2021-05-28T19:41:48.521628Z","shell.execute_reply":"2021-05-28T19:41:54.518187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca_groups = [['smog', 'syll_per_word', 'spacy_29'], ['coleman_liau', 'nominalization', 'IN'], ['spacy_68', 'spacy_86', 'spacy_208', 'spacy_262', 'spacy_147', 'spacy_261'], ['spacy_110', 'spacy_114', 'spacy_298', 'spacy_269', 'spacy_151'], ['spacy_76', 'spacy_122', 'periods', 'spacy_72', 'spacy_196'], ['spacy_4', 'spacy_214', 'spacy_101', 'flesch', 'periods'], ['pronoun', 'spacy_269', 'spacy_294', 'spacy_151', 'spacy_147', 'spacy_110', 'spacy_196'], ['spacy_264', 'spacy_134', 'spacy_122', 'spacy_86', 'spacy_254', 'spacy_72'], ['spacy_76', 'spacy_114', 'spacy_298', 'spacy_69'], ['spacy_28', 'spacy_269', 'spacy_151', 'spacy_122', 'spacy_72', 'spacy_69', 'spacy_134', 'spacy_9', 'spacy_254'], ['spacy_101', 'spacy_214', 'spacy_262', 'spacy_89', 'spacy_110', 'spacy_208'], ['spacy_86', 'spacy_105', 'spacy_249', 'spacy_294', 'VBD', 'spacy_147', 'flesch', 'periods'], ['spacy_28', 'pronoun', 'spacy_122', 'spacy_101', 'spacy_110', 'periods', 'spacy_9'], ['spacy_249', 'PRP', 'spacy_68', 'spacy_294', 'VBD', 'spacy_261', 'spacy_4', 'spacy_298'], ['spacy_76', 'spacy_72', 'spacy_208', 'spacy_89', 'flesch', 'spacy_196', 'spacy_69'], ['spacy_249', 'spacy_68', 'spacy_76', 'spacy_122', 'spacy_208', 'spacy_214', 'spacy_101', 'spacy_254'], ['PRP', 'spacy_114', 'spacy_28', 'spacy_151', 'spacy_4', 'spacy_110', 'spacy_279', 'spacy_232'], ['VBD', 'spacy_264', 'spacy_134', 'spacy_269', 'spacy_261'], ['spacy_249', 'spacy_110', 'spacy_68', 'spacy_9', 'spacy_122', 'spacy_28', 'spacy_147'], ['spacy_269', 'PRP', 'spacy_151', 'spacy_298', 'spacy_101', 'spacy_198', 'spacy_72'], ['spacy_198', 'spacy_28', 'spacy_151', 'spacy_269', 'spacy_261'], ['spacy_249', 'PRP', 'spacy_114', 'spacy_122', 'spacy_110', 'spacy_264', 'spacy_208', 'spacy_133'], ['spacy_214', 'PRP', 'spacy_114', 'spacy_86', 'periods', 'spacy_28'], ['pronoun', 'spacy_76', 'spacy_68', 'spacy_262', 'VBD', 'spacy_122', 'spacy_105', 'spacy_298'], ['spacy_151', 'spacy_134', 'spacy_269', 'spacy_279', 'periods', 'spacy_89', 'spacy_133', 'spacy_147', 'spacy_232'], ['spacy_249', 'spacy_86', 'PRP', 'spacy_114', 'spacy_122', 'spacy_69', 'spacy_294', 'spacy_68', 'spacy_254', 'spacy_110'], ['PRP', 'spacy_214', 'pronoun', 'VBD', 'spacy_114', 'spacy_254', 'spacy_294', 'spacy_261', 'spacy_208', 'spacy_134', 'spacy_4', 'spacy_89', 'spacy_298'], ['spacy_269', 'spacy_249', 'spacy_151', 'spacy_76', 'spacy_122', 'spacy_101', 'periods'], ['spacy_294', 'spacy_214', 'spacy_76', 'spacy_28', 'spacy_86', 'spacy_264', 'spacy_232', 'spacy_122', 'spacy_9'], ['spacy_294', 'spacy_68', 'spacy_122', 'spacy_4', 'spacy_264', 'spacy_261', 'spacy_196', 'spacy_9'], ['spacy_114', 'spacy_249', 'spacy_86', 'spacy_151', 'spacy_134', 'spacy_101', 'spacy_76', 'spacy_254', 'spacy_262'], ['spacy_249', 'PRP', 'pronoun', 'spacy_28', 'spacy_269', 'spacy_114', 'spacy_68', 'spacy_294', 'spacy_122', 'spacy_261', 'spacy_196', 'spacy_72', 'spacy_133'], ['spacy_86', 'spacy_76', 'flesch', 'spacy_4', 'spacy_89', 'spacy_110', 'spacy_9', 'spacy_151'], ['spacy_110', 'spacy_86', 'spacy_208', 'spacy_214', 'spacy_134'], ['spacy_114', 'spacy_269', 'spacy_249', 'spacy_101', 'spacy_76', 'spacy_198', 'spacy_9', 'flesch'], ['spacy_122', 'spacy_294', 'spacy_72'], ['spacy_298', 'spacy_214', 'VBD', 'spacy_114', 'spacy_264', 'spacy_68', 'spacy_9', 'spacy_134', 'spacy_262', 'spacy_4', 'spacy_147', 'flesch'], ['spacy_249', 'spacy_69', 'spacy_105', 'spacy_89', 'spacy_110'], ['preposition', 'smog'], ['spacy_14', 'chars_per_word', 'spacy_29'], ['IN', 'spacy_107'], ['spacy_160', 'nominalization', 'spacy_2'], ['syll_per_word', 'NN', 'spacy_200'], ['coleman_liau', 'NN', 'spacy_60', 'spacy_263'], ['spacy_14', 'dale_chall', 'JJ', 'spacy_182'], ['nominalization', 'spacy_46', 'spacy_155', 'spacy_107'], ['spacy_203', 'smog', 'spacy_149', 'spacy_24'], ['spacy_103', 'avg_len_word', 'num_char', 'spacy_200'], ['spacy_146', 'rix'], ['syll_per_word', 'IN', 'spacy_159', 'spacy_10'], ['preposition', 'spacy_29', 'syll_per_word', 'IN', 'spacy_10'], ['IN', 'spacy_29', 'num_char', 'spacy_182', 'spacy_240', 'spacy_10'], ['chars_per_word', 'avg_len_word', 'nominalization', 'rix', 'spacy_149', 'JJ', 'spacy_2'], ['smog', 'spacy_60', 'spacy_146', 'spacy_97', 'spacy_162'], ['spacy_50', 'spacy_14', 'spacy_38', 'spacy_192', 'dale_chall', 'spacy_24'], ['spacy_203', 'coleman_liau', 'spacy_107', 'preposition'], ['spacy_14', 'coleman_liau', 'spacy_43', 'spacy_182', 'spacy_258'], ['smog', 'rix', 'dale_chall', 'spacy_197', 'spacy_251', 'spacy_107', 'spacy_240'], ['spacy_160', 'IN', 'spacy_27', 'spacy_192', 'JJ', 'nominalization'], ['chars_per_word', 'spacy_7', 'spacy_148', 'spacy_97', 'spacy_159', 'spacy_217', 'lix', 'spacy_200'], ['spacy_155', 'spacy_203', 'avg_len_word', 'spacy_60', 'spacy_252', 'NN', 'spacy_146', 'gunning_fog'], ['syll_per_word', 'num_char', 'spacy_38', 'spacy_24', 'preposition'], ['coleman_liau', 'spacy_2', 'spacy_160', 'spacy_107', 'spacy_162'], ['smog', 'spacy_43', 'spacy_197', 'dale_chall', 'spacy_155'], ['spacy_29', 'IN', 'chars_per_word', 'spacy_252', 'spacy_24', 'preposition', 'lix', 'spacy_200', 'spacy_50'], ['spacy_14', 'syll_per_word', 'spacy_7', 'spacy_27', 'nominalization', 'spacy_211', 'gunning_fog', 'spacy_10'], ['avg_len_word', 'spacy_251', 'rix', 'spacy_30', 'spacy_217', 'spacy_149'], ['spacy_97', 'spacy_149', 'spacy_200', 'spacy_263', 'spacy_162', 'spacy_182', 'gunning_fog'], ['spacy_14', 'spacy_203', 'spacy_43', 'JJ', 'spacy_30', 'dale_chall', 'spacy_146', 'spacy_50', 'spacy_10'], ['coleman_liau', 'spacy_251', 'nominalization', 'spacy_7', 'spacy_46', 'spacy_155', 'preposition', 'spacy_240', 'spacy_266', 'lix'], ['chars_per_word', 'syll_per_word', 'smog', 'IN', 'spacy_38', 'spacy_107', 'spacy_2'], ['nominalization', 'spacy_251', 'IN', 'spacy_155', 'NN', 'spacy_24', 'gunning_fog'], ['smog', 'coleman_liau', 'spacy_14', 'spacy_146', 'spacy_266', 'spacy_240', 'spacy_182'], ['syll_per_word', 'spacy_43', 'spacy_197', 'spacy_46', 'spacy_217', 'spacy_38', 'dale_chall', 'spacy_192', 'rix', 'spacy_200', 'spacy_162'], ['num_char', 'spacy_29', 'lix', 'spacy_10'], ['avg_len_word', 'spacy_10', 'preposition'], ['spacy_217', 'spacy_149', 'spacy_97', 'spacy_24', 'num_char', 'syll_per_word', 'spacy_266'], ['spacy_252', 'spacy_29', 'spacy_14', 'spacy_30', 'spacy_211', 'spacy_192', 'dale_chall', 'spacy_107', 'spacy_60', 'lix'], ['coleman_liau', 'smog', 'spacy_197', 'spacy_251', 'rix', 'spacy_27'], ['spacy_103', 'spacy_160', 'chars_per_word', 'spacy_7', 'spacy_146', 'spacy_240'], ['chars_per_word', 'spacy_29', 'spacy_30', 'spacy_217', 'spacy_197', 'spacy_155', 'spacy_24', 'spacy_97', 'spacy_162', 'spacy_182', 'spacy_10'], ['coleman_liau', 'avg_len_word', 'smog', 'spacy_258', 'spacy_7', 'preposition', 'JJ', 'spacy_240'], ['IN', 'num_char', 'spacy_38', 'lix'], ['spacy_29', 'spacy_2', 'dale_chall', 'spacy_203', 'spacy_263', 'spacy_107', 'spacy_160'], ['coleman_liau', 'syll_per_word', 'avg_len_word', 'nominalization', 'spacy_103', 'num_char', 'spacy_38', 'spacy_192', 'JJ'], ['chars_per_word', 'spacy_252', 'spacy_14', 'spacy_197', 'spacy_211', 'spacy_182', 'spacy_240', 'lix', 'gunning_fog', 'spacy_266'], ['chars_per_word', 'spacy_27', 'spacy_149', 'spacy_197', 'NN', 'preposition', 'spacy_97', 'gunning_fog'], ['nominalization', 'rix', 'spacy_14', 'num_char'], ['coleman_liau', 'syll_per_word', 'avg_len_word', 'smog', 'spacy_103', 'JJ', 'spacy_251', 'dale_chall'], ['spacy_43', 'spacy_2', 'spacy_103', 'spacy_192', 'spacy_240', 'spacy_200', 'spacy_146', 'spacy_149', 'spacy_24', 'spacy_107', 'spacy_263'], ['coleman_liau', 'avg_len_word', 'nominalization', 'smog', 'spacy_7', 'rix', 'spacy_46', 'spacy_29', 'spacy_30', 'spacy_162', 'lix', 'gunning_fog', 'spacy_10'], ['spacy_251', 'spacy_38', 'spacy_60', 'dale_chall', 'num_char', 'spacy_252', 'spacy_266', 'JJ', 'spacy_217', 'spacy_24', 'spacy_182'], ['nominalization', 'spacy_97', 'spacy_148', 'spacy_240'], ['spacy_7', 'spacy_29', 'spacy_14', 'spacy_43', 'spacy_2'], ['coleman_liau', 'chars_per_word', 'syll_per_word', 'rix', 'spacy_146', 'spacy_197', 'spacy_192'], ['spacy_148', 'spacy_258', 'spacy_7', 'spacy_149', 'rix', 'spacy_107', 'dale_chall'], ['syll_per_word', 'avg_len_word', 'smog', 'nominalization', 'spacy_263', 'spacy_203', 'spacy_211', 'preposition', 'spacy_200', 'spacy_162'], ['spacy_251', 'spacy_46', 'spacy_43', 'IN', 'spacy_155', 'gunning_fog'], ['spacy_29', 'spacy_30', 'spacy_38', 'JJ', 'spacy_107', 'spacy_162', 'lix', 'gunning_fog'], ['coleman_liau', 'spacy_46', 'spacy_252', 'spacy_14', 'spacy_251', 'spacy_200', 'nominalization', 'rix', 'num_char', 'spacy_155'], ['syll_per_word', 'chars_per_word', 'spacy_146', 'spacy_217', 'spacy_159', 'spacy_266'], ['chars_per_word', 'syll_per_word', 'IN', 'spacy_146', 'smog'], ['rix', 'spacy_14', 'spacy_263'], ['coleman_liau', 'avg_len_word', 'spacy_43', 'nominalization', 'spacy_203', 'dale_chall'], ['spacy_203', 'spacy_2', 'dale_chall', 'spacy_217', 'spacy_266', 'preposition', 'spacy_159', 'spacy_162', 'lix'], ['num_char', 'spacy_149', 'JJ', 'spacy_46', 'gunning_fog'], ['syll_per_word', 'coleman_liau', 'chars_per_word', 'smog', 'spacy_43', 'nominalization', 'num_char', 'dale_chall', 'spacy_197', 'spacy_192', 'spacy_149', 'spacy_97', 'spacy_146', 'lix'], ['spacy_27', 'rix', 'spacy_103', 'IN', 'spacy_162', 'NN', 'gunning_fog', 'spacy_10'], ['spacy_107', 'preposition', 'spacy_266'], ['chars_per_word', 'avg_len_word', 'smog', 'spacy_103', 'nominalization', 'spacy_46', 'spacy_14'], ['spacy_155', 'spacy_97', 'spacy_27', 'spacy_107', 'spacy_182', 'lix', 'gunning_fog', 'spacy_266'], ['coleman_liau', 'chars_per_word', 'nominalization', 'rix', 'smog', 'spacy_263', 'spacy_2'], ['syll_per_word', 'avg_len_word', 'spacy_103', 'spacy_149'], ['spacy_27', 'dale_chall', 'spacy_211', 'NN', 'spacy_24'], ['spacy_155', 'spacy_27', 'dale_chall', 'spacy_211', 'NN', 'avg_len_word', 'spacy_162', 'spacy_24'], ['spacy_14', 'avg_len_word', 'spacy_160', 'spacy_29', 'spacy_197', 'spacy_30', 'spacy_155', 'spacy_27', 'dale_chall']]\n\nfor index, group in enumerate(pca_groups):\n    key = 'f'+str(index)\n    pca_res, train = train_pca(train, group, key)\n    test = apply_pca(pca_res, test)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T19:41:54.522042Z","iopub.execute_input":"2021-05-28T19:41:54.522301Z","iopub.status.idle":"2021-05-28T19:41:58.76979Z","shell.execute_reply.started":"2021-05-28T19:41:54.522276Z","shell.execute_reply":"2021-05-28T19:41:58.768853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ignore_cols = ['id','url_legal','license','excerpt', 'standard_error', 'target']\n\nfor col in train.select_dtypes('object').columns.tolist():\n    if col not in ignore_cols:\n        lbl = LabelEncoder()\n        train[col] = lbl.fit_transform(train[col])\n        test[col] = lbl.transform(test[col])","metadata":{"execution":{"iopub.status.busy":"2021-05-28T19:41:58.774998Z","iopub.execute_input":"2021-05-28T19:41:58.777304Z","iopub.status.idle":"2021-05-28T19:41:58.800824Z","shell.execute_reply.started":"2021-05-28T19:41:58.777261Z","shell.execute_reply":"2021-05-28T19:41:58.799968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(\"GPU is available\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"GPU not available, CPU used\")","metadata":{"execution":{"iopub.status.busy":"2021-05-28T19:41:58.8052Z","iopub.execute_input":"2021-05-28T19:41:58.80745Z","iopub.status.idle":"2021-05-28T19:41:58.881565Z","shell.execute_reply.started":"2021-05-28T19:41:58.807406Z","shell.execute_reply":"2021-05-28T19:41:58.880606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_kfolds(df,target_col, seed):\n\n    df[\"kfold\"] = -1\n\n    df = df.sample(frac=1).reset_index(drop=True)\n\n    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=seed)\n\n    for fold, (train_idx, val_idx) in enumerate(kf.split(X=df)):\n        print(len(train_idx), len(val_idx))\n        df.loc[val_idx, 'kfold'] = fold\n\n    return df\n\ndef create_Stratkfolds(df,target_col, seed):\n\n    df[\"kfold\"] = -1\n\n    df = df.sample(frac=1).reset_index(drop=True)\n\n    ### This was taken from https://www.kaggle.com/abhishek/step-1-create-folds\n    # calculate number of bins by Sturge's rule\n    # I take the floor of the value, you can also\n    # just round it\n    num_bins = int(np.floor(1 + np.log2(len(df))))\n    \n    # bin targets\n    df.loc[:, \"bins\"] = pd.cut(\n        df[target_col], bins=num_bins, labels=False\n    )\n\n    kf = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n\n    for fold, (train_idx, val_idx) in enumerate(kf.split(X=df, y = df.bins.values)):\n        print(len(train_idx), len(val_idx))\n        df.loc[val_idx, 'kfold'] = fold\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-05-28T19:41:58.882992Z","iopub.execute_input":"2021-05-28T19:41:58.883393Z","iopub.status.idle":"2021-05-28T19:41:58.893921Z","shell.execute_reply.started":"2021-05-28T19:41:58.883353Z","shell.execute_reply":"2021-05-28T19:41:58.892819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ReadabiltyDataset(nn.Module):\n    def __init__(self, data, tokenizer):\n        self.sentences = data[config.TEXT_COLUMN].to_numpy()\n        self.tokenizer = tokenizer\n        self.max_len = config.MAX_LEN\n\n    def __len__(self):\n        return len(self.sentences)\n\n    def __getitem__(self, idx):\n\n        encode = self.tokenizer(self.sentences[idx],\n            return_tensors='pt',\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True\n        )\n\n        return encode","metadata":{"execution":{"iopub.status.busy":"2021-05-28T19:41:58.895434Z","iopub.execute_input":"2021-05-28T19:41:58.895862Z","iopub.status.idle":"2021-05-28T19:41:58.905047Z","shell.execute_reply.started":"2021-05-28T19:41:58.895825Z","shell.execute_reply":"2021-05-28T19:41:58.904167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Thanks to this kernel to help me load the embeddings : https://www.kaggle.com/lars123/neural-tangent-kernel-2\ndef get_embeddings(df,path,plot_losses=True, verbose=True):\n            \n    MODEL_PATH = path\n    model = AutoModel.from_pretrained(MODEL_PATH)\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n    model.to(device)\n    model.eval()\n\n    ds = ReadabiltyDataset(df,tokenizer)\n    dl = DataLoader(ds,\n                  batch_size = config.TRAIN_BATCH_SIZE,\n                  shuffle=False,\n                  num_workers = 4,\n                  pin_memory=True,\n                  drop_last=False)\n        \n    embeddings = list()\n    with torch.no_grad():\n        for i, inputs in tqdm.tqdm(enumerate(dl)):\n            inputs = {key:val.reshape(val.shape[0],-1).to(device) for key,val in inputs.items()}\n            outputs = model(**inputs)\n            outputs = outputs[0][:,0].detach().cpu().numpy()\n            embeddings.extend(outputs)\n    return np.array(embeddings)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T19:41:58.906388Z","iopub.execute_input":"2021-05-28T19:41:58.9069Z","iopub.status.idle":"2021-05-28T19:41:58.919199Z","shell.execute_reply.started":"2021-05-28T19:41:58.90686Z","shell.execute_reply":"2021-05-28T19:41:58.918309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_embeddings1 =  get_embeddings(train,'../input/modelf1')\ntest_embeddings1 = get_embeddings(test,'../input/modelf1')\n\ntrain_embeddings2 =  get_embeddings(train,'../input/modelf2')\ntest_embeddings2 = get_embeddings(test,'../input/modelf2')\n\ntrain_embeddings3 =  get_embeddings(train,'../input/modelf3')\ntest_embeddings3 = get_embeddings(test,'../input/modelf3')\n\ntrain_embeddings4 =  get_embeddings(train,'../input/modelf4')\ntest_embeddings4 = get_embeddings(test,'../input/modelf4')\n\ntrain_embeddings5 =  get_embeddings(train,'../input/modelf5')\ntest_embeddings5 = get_embeddings(test,'../input/modelf5')","metadata":{"execution":{"iopub.status.busy":"2021-05-28T19:41:58.920622Z","iopub.execute_input":"2021-05-28T19:41:58.921002Z","iopub.status.idle":"2021-05-28T19:44:23.595128Z","shell.execute_reply.started":"2021-05-28T19:41:58.920965Z","shell.execute_reply":"2021-05-28T19:44:23.593691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.drop(columns=ignore_cols, inplace=True, axis=1)\nignore_cols.remove('standard_error')\nignore_cols.remove('target')\ntest.drop(columns=ignore_cols, inplace=True, axis=1)\n\npca_columns = []\nfor i in range(116):\n    pca_columns.append('f'+str(i)+'_1')\n    pca_columns.append('f'+str(i)+'_2')\n\ndrop_cols = [i for i in train_data.columns if i not in pca_columns]\n\ntrain_data.drop(columns=drop_cols, inplace=True, axis=1)\ntest_data.drop(columns=drop_cols, inplace=True, axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T19:44:23.597048Z","iopub.execute_input":"2021-05-28T19:44:23.597433Z","iopub.status.idle":"2021-05-28T19:44:23.616581Z","shell.execute_reply.started":"2021-05-28T19:44:23.59739Z","shell.execute_reply":"2021-05-28T19:44:23.615277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embed_columns = []\nfor i in range(768):\n    embed_columns.append('E_'+str(i))\n\ntrain_embeddings_1_df = pd.DataFrame(data=train_embeddings1, columns=embed_columns)\ntrain_embeddings_2_df = pd.DataFrame(data=train_embeddings2, columns=embed_columns)\ntrain_embeddings_3_df = pd.DataFrame(data=train_embeddings3, columns=embed_columns)\ntrain_embeddings_4_df = pd.DataFrame(data=train_embeddings4, columns=embed_columns)\ntrain_embeddings_5_df = pd.DataFrame(data=train_embeddings5, columns=embed_columns)\ntest_embeddings_1_df = pd.DataFrame(data=test_embeddings1, columns=embed_columns)\ntest_embeddings_2_df = pd.DataFrame(data=test_embeddings2, columns=embed_columns)\ntest_embeddings_3_df = pd.DataFrame(data=test_embeddings3, columns=embed_columns)\ntest_embeddings_4_df = pd.DataFrame(data=test_embeddings4, columns=embed_columns)\ntest_embeddings_5_df = pd.DataFrame(data=test_embeddings5, columns=embed_columns)\n\ntrain1 = pd.merge(train, train_embeddings_1_df, left_index=True, right_index=True)\ntrain2 = pd.merge(train, train_embeddings_2_df, left_index=True, right_index=True)\ntrain3 = pd.merge(train, train_embeddings_3_df, left_index=True, right_index=True)\ntrain4 = pd.merge(train, train_embeddings_4_df, left_index=True, right_index=True)\ntrain5 = pd.merge(train, train_embeddings_5_df, left_index=True, right_index=True)\ntest1 = pd.merge(test, test_embeddings_1_df, left_index=True, right_index=True)\ntest2 = pd.merge(test, test_embeddings_2_df, left_index=True, right_index=True)\ntest3 = pd.merge(test, test_embeddings_3_df, left_index=True, right_index=True)\ntest4 = pd.merge(test, test_embeddings_4_df, left_index=True, right_index=True)\ntest5 = pd.merge(test, test_embeddings_5_df, left_index=True, right_index=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T19:44:23.618115Z","iopub.execute_input":"2021-05-28T19:44:23.618501Z","iopub.status.idle":"2021-05-28T19:44:23.819544Z","shell.execute_reply.started":"2021-05-28T19:44:23.618464Z","shell.execute_reply":"2021-05-28T19:44:23.818519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def runLGB_reg(train_X, train_y, test_X, test_y=None, test_X2=None, dep=8, seed=0, data_leaf=50, rounds=20000):\n    params = {}\n    params[\"objective\"] = \"regression\"\n    params['metric'] = 'rmse'\n    params[\"max_depth\"] = dep\n    params[\"num_leaves\"] = 30\n    params[\"min_data_in_leaf\"] = data_leaf\n    #     params[\"min_sum_hessian_in_leaf\"] = 50\n    params[\"learning_rate\"] = 0.01\n    params[\"bagging_fraction\"] = 0.8\n    params[\"feature_fraction\"] = 0.2\n    params[\"feature_fraction_seed\"] = seed\n    params[\"bagging_freq\"] = 1\n    params[\"bagging_seed\"] = seed\n    params[\"lambda_l2\"] = 3\n    params[\"lambda_l1\"] = 3\n    params[\"verbosity\"] = -1\n#     params[\"sample_weight\"] = sample_weight\n    num_rounds = rounds\n\n    plst = list(params.items())\n    lgtrain = lgb.Dataset(train_X, label=train_y)\n\n    if test_y is not None:\n        lgtest = lgb.Dataset(test_X, label=test_y)\n        model = lgb.train(params, lgtrain, num_rounds, valid_sets=[lgtest], early_stopping_rounds=200, verbose_eval=500)\n\n    #         model = lgb.LGBMRegressor()\n    else:\n        lgtest = lgb.DMatrix(test_X)\n        model = lgb.train(params, lgtrain, num_rounds)\n\n    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n    pred_test_y2 = model.predict(test_X2, num_iteration=model.best_iteration)\n    #imps = model.feature_importance()\n    #names = model.feature_name()\n    #for fi, fn in enumerate(names):\n    #    print(fn, imps[fi])\n\n    loss = 0\n    if test_y is not None:\n        loss = np.sqrt(metrics.mean_squared_error(test_y, pred_test_y))\n        return model, loss, pred_test_y, pred_test_y2\n    else:\n        return model, loss, pred_test_y, pred_test_y2","metadata":{"execution":{"iopub.status.busy":"2021-05-28T19:44:23.820976Z","iopub.execute_input":"2021-05-28T19:44:23.821346Z","iopub.status.idle":"2021-05-28T19:44:23.834278Z","shell.execute_reply.started":"2021-05-28T19:44:23.821307Z","shell.execute_reply":"2021-05-28T19:44:23.83338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_lgb(X_train, y_train, test_X):\n    print(\"Building model..\")\n    cv_scores = []\n    pred_test_full = 0\n    pred_train = np.zeros(X_train.shape[0])\n    n_splits = 5\n    kf = model_selection.KFold(n_splits=n_splits, shuffle=True, random_state=7988)\n    # gkf = model_selection.GroupKFold(n_splits=n_splits)\n    model_name = \"lgb\"\n    for dev_index, val_index in kf.split(X_train, y_train):\n        dev_X, val_X = X_train.iloc[dev_index,:], X_train.iloc[val_index,:]\n        dev_y, val_y = y_train[dev_index], y_train[val_index]\n\n        pred_val = 0\n        pred_test = 0\n        n_models = 0.\n\n        model, loss, pred_v, pred_t = runLGB_reg(dev_X, dev_y, val_X, val_y, test_X, dep=6, data_leaf=200, seed=2019)\n        pred_val += pred_v\n        pred_test += pred_t\n        n_models += 1\n\n        model, loss, pred_v, pred_t = runLGB_reg(dev_X, dev_y, val_X, val_y, test_X,  dep=7, data_leaf=180, seed=9873)\n        pred_val += pred_v\n        pred_test += pred_t\n        n_models += 1\n\n    #     model, loss, pred_v, pred_t = runLGB(dev_X, dev_y, val_X, val_y, test_X, dep=7, data_leaf=200, seed=4568)\n    #     pred_val += pred_v\n    #     pred_test += pred_t\n    #     n_models += 1\n\n\n        pred_val /= n_models\n        pred_test /= n_models\n\n        loss = np.sqrt(metrics.mean_squared_error(val_y, pred_val))\n\n        pred_train[val_index] = pred_val\n        pred_test_full += pred_test / n_splits\n        cv_scores.append(loss)\n    #     break\n    print(np.mean(cv_scores))\n    return pred_test","metadata":{"execution":{"iopub.status.busy":"2021-05-28T19:44:23.835975Z","iopub.execute_input":"2021-05-28T19:44:23.83637Z","iopub.status.idle":"2021-05-28T19:44:23.847353Z","shell.execute_reply.started":"2021-05-28T19:44:23.83633Z","shell.execute_reply":"2021-05-28T19:44:23.846467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_1 = run_lgb(train1,target,test1)\npreds_2 = run_lgb(train2,target,test2)\npreds_3 = run_lgb(train3,target,test3)\npreds_4 = run_lgb(train4,target,test4)\npreds_5 = run_lgb(train5,target,test5)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T19:44:23.848756Z","iopub.execute_input":"2021-05-28T19:44:23.84917Z","iopub.status.idle":"2021-05-28T19:54:10.023381Z","shell.execute_reply.started":"2021-05-28T19:44:23.849132Z","shell.execute_reply":"2021-05-28T19:54:10.022462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_preds = (preds_1 + preds_2 + preds_3 + preds_4 + preds_5)/5","metadata":{"execution":{"iopub.status.busy":"2021-05-28T19:54:10.024955Z","iopub.execute_input":"2021-05-28T19:54:10.025308Z","iopub.status.idle":"2021-05-28T19:54:10.030225Z","shell.execute_reply.started":"2021-05-28T19:54:10.025271Z","shell.execute_reply":"2021-05-28T19:54:10.029273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.DataFrame({'id': sample.id, 'target': final_preds})\nsubmission_df","metadata":{"execution":{"iopub.status.busy":"2021-05-28T19:54:10.03144Z","iopub.execute_input":"2021-05-28T19:54:10.032011Z","iopub.status.idle":"2021-05-28T19:54:10.055113Z","shell.execute_reply.started":"2021-05-28T19:54:10.031975Z","shell.execute_reply":"2021-05-28T19:54:10.054132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T19:54:10.056447Z","iopub.execute_input":"2021-05-28T19:54:10.056837Z","iopub.status.idle":"2021-05-28T19:54:10.089204Z","shell.execute_reply.started":"2021-05-28T19:54:10.056799Z","shell.execute_reply":"2021-05-28T19:54:10.088459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}